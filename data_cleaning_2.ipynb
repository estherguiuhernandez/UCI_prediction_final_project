{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c712bbe5-041d-4978-8dd2-f3021db337eb",
   "metadata": {},
   "source": [
    "### DATA CLEANING - 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee957b49-bd3f-415f-991b-3b05e807303c",
   "metadata": {},
   "source": [
    "Please read data_cleaning_updated for details about the initial steps for data analysis.\n",
    "After running the models and investigatig further the data the following decisions were taken:\n",
    "\n",
    "<b> Feature extraction: </b>\n",
    "\n",
    "- Remove outliers by calculating 6 * IQR. The reason behind this is that normally an outlier is consider as something that is above 1.5 IQR. In this case because we have ill patients we consider bigger IQR as plausible values.\n",
    "- Create a feature BMI and remove from the dataframe Weight and Heigh\n",
    "- For Mechanical ventilation assume Nan as no mechanical ventilation (assign this values to 0)\n",
    "- For every time parameter extract:\n",
    "   - median\n",
    "   - median at 24 hours\n",
    "   - median at 48 hours\n",
    "   - normalized MAD\n",
    "   - max\n",
    "   - last value\n",
    "   - slope\n",
    "\n",
    "<b> Feature selection: </b> \n",
    "- Looking for features that are not representative and with excessive NaN values: Eliminate features were more than 50% of participants have missing values\n",
    "\n",
    "- Look at correlations amongst features and also correlations amongst features and eliminate those features with a correlation > 0.8 \n",
    "\n",
    "- Drop low variance features:  Zero or near-zero variance. Features that are (almost) constant provide little information to learn from and thus are irrelevant.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "6a919cb4-e9d8-49ee-a691-72057d557685",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import glob\n",
    "from pathlib import Path  # Importing the Path class from the pathlib module\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import os \n",
    "import math\n",
    "import seaborn as sns\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.preprocessing import RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "eccbc77a-d252-40a6-835f-bb257ad452f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GLOBAL VARIABLES\n",
    "# Define the programmer whose path you want to use\n",
    "programmer = 'esther'\n",
    "\n",
    "#variables for paths names\n",
    "file_paths_names = {\n",
    "    'patricia': r\"..\\original_data\",\n",
    "    'esther': r\"C:\\Users\\egh22\\OneDrive - University of Canterbury\\bootcamp\\group project\\data\"\n",
    "}\n",
    "\n",
    "files_outcome_path_names = {\n",
    "    'patricia': r\"..\\original_data\\outcomes\",\n",
    "    'esther': r\"C:\\Users\\egh22\\OneDrive - University of Canterbury\\bootcamp\\group project\\data\\outcomes\"\n",
    "}\n",
    "\n",
    "\n",
    "files_path_to_save_files = {\n",
    "    'patricia': r\"..\\clean_data\",\n",
    "    'esther': r\"C:\\Users\\egh22\\OneDrive - University of Canterbury\\bootcamp\\group project\\clean_data\"\n",
    "}\n",
    "\n",
    "# Choose the file path based on the programmer variable\n",
    "file_path_to_save = files_path_to_save_files[programmer]\n",
    "\n",
    "\n",
    "# VARIABLES FOR FEATURE REDUCTION\n",
    "#threshold variables\n",
    "#defines the number of std needed for a value to be considered out of range\n",
    "threshold_for_outliers = 6\n",
    "#defines the percentage of patients with NaNs for a feature  to be deleted from dataframe\n",
    "thresholds_for_non_important_features = 50\n",
    "#variance reduction threshold \n",
    "thresholds_variance_reduction = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "89e238d8-1cc4-4d8e-86ff-9d115a164b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeit(func):\n",
    "    \"\"\"\n",
    "    Decorator function to measure the execution time of another function.\n",
    "\n",
    "    Parameters:\n",
    "        func (callable): The function to be decorated.\n",
    "\n",
    "    Returns:\n",
    "        callable: A wrapper function that measures the execution time of the decorated function.\n",
    "    \"\"\"\n",
    "    def wrapper(*args, **kwargs):\n",
    "        \"\"\"\n",
    "        Wrapper function that measures the execution time of the decorated function.\n",
    "\n",
    "        Parameters:\n",
    "            *args: Positional arguments to be passed to the decorated function.\n",
    "            **kwargs: Keyword arguments to be passed to the decorated function.\n",
    "\n",
    "        Returns:\n",
    "            Any: The return value of the decorated function.\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        print(f\"Function '{func.__name__}' elapsed time: {elapsed_time} seconds\")\n",
    "        return result\n",
    "    return wrapper\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d09a21-eae5-41ce-b54b-1b44f591d02d",
   "metadata": {},
   "source": [
    "### 1. Feature extraction\n",
    "\n",
    "For every time parameter extract:\n",
    "   - median\n",
    "   - median at 24 hours\n",
    "   - median at 48 hours\n",
    "   - normalized MAD\n",
    "   - max\n",
    "   - last value\n",
    "   - slope\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "454a6a51-f535-4018-98a7-0057745efeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeit\n",
    "def process_time_series_files(time_series_files: list, all_parameters: list, non_time_parameters: list, categorical_parameters:list, summary_function: callable) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Process a list of time series files and extract summary statistics for each file using a specified summary function.\n",
    "\n",
    "    Parameters:\n",
    "        time_series_files (list): A list of file paths to time series data files.\n",
    "        all_parameters (list): A list of all parameters to calculate metrics for.\n",
    "        non_time_parameters (list): A list of non-time parameters to extract values for.\n",
    "        categorical_parameters (list): A list of special non-time parameters, that may appear more than once over time but always takes same value to extract values for.\n",
    "        summary_function (callable): A function to calculate summary statistics for a DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the summary statistics for all files.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for file_path in time_series_files:\n",
    "        # Read the file into a DataFrame\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Extract summary statistics for the DataFrame using the specified summary function\n",
    "        result = summary_function(df, all_parameters, non_time_parameters, categorical_parameters)\n",
    "        \n",
    "        # Append the result to the results list\n",
    "        results.append(result)\n",
    "\n",
    "    # Concatenate results into a single DataFrame\n",
    "    final_result = pd.concat(results, ignore_index=True)\n",
    "    \n",
    "    return final_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "4f390463-94db-404f-89a8-66fff29d366e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a list with all time parameters in the dataset\n",
    "time_parameters =  ['Albumin', 'ALP', 'ALT', 'AST', 'Bilirubin', 'BUN', 'Cholesterol', \n",
    "                    'Creatinine', 'DiasABP', 'FiO2', 'GCS', 'Glucose', 'HCO3', 'HCT', 'HR', 'K',\n",
    "                    'Lactate', 'Mg', 'MAP', 'Na', 'NIDiasABP', 'NIMAP', 'NISysABP', 'PaCO2', \n",
    "                 'PaO2',  'pH', 'Platelets', 'RespRate', 'SaO2', 'SysABP', 'Temp', 'TropI', 'TropT', 'Urine', 'WBC',  'Weight']\n",
    "\n",
    "non_time_parameters = ['RecordID', 'Age', 'Gender', 'Height', 'ICUType']\n",
    "demographic_parameters = non_time_parameters\n",
    "\n",
    "categorical_parameters = ['MechVent']\n",
    "\n",
    "# Concatenate all the lists\n",
    "all_parameters = time_parameters + non_time_parameters + categorical_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "a6c5bcef-50df-4872-b926-94deff357f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of text files: 8000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\egh22\\\\OneDrive - University of Canterbury\\\\bootcamp\\\\group project\\\\data\\\\132540.txt'"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify the file path using a raw string\n",
    "#Modify path to your desired one\n",
    "files_path = file_paths_names[programmer]\n",
    "\n",
    "# Use glob to find all .txt files in the directory\n",
    "time_series_files = glob.glob(str(Path(files_path) / \"*.txt\"))  # Using Path to construct the file path\n",
    "\n",
    "# Print the number of files found\n",
    "print(f\"Number of text files: {len(time_series_files)}\")\n",
    "#delete this after demonstration\n",
    "time_series_files_test = time_series_files[1]\n",
    "#df = pd.read_csv(time_series_files[1])\n",
    "time_series_files_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "6cc14c57-f7bd-4973-9675-bd617e7c7fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read target variables\n",
    "\n",
    "#modify this with the name of the paths to read\n",
    "files_outcome_path =  files_outcome_path_names[programmer]\n",
    "df_target_a = pd.read_csv(Path(files_outcome_path) / \"Outcomes-a.txt\")\n",
    "df_target_b = pd.read_csv(Path(files_outcome_path) / \"Outcomes-b.txt\")\n",
    "\n",
    "outcomes_df = pd.concat([df_target_a, df_target_b], ignore_index=True)\n",
    "outcome_columns = outcomes_df.columns.tolist()\n",
    "outcome_columns.remove('RecordID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "a62076cc-a8f6-4256-8025-b61443a480ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_summarysed_features(df, time_parameters_filtered, demographic_parameters, categorical_parameters):\n",
    "    \"\"\"\n",
    "    Generate new time summarised features based on different statistics and extract the values for demographic parameters, categorical parameters, \n",
    "    from the given DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input DataFrame containing time-series data.\n",
    "        time_parameters_filtered (list): A list of time-related parameters to extract statistics for.\n",
    "        demographic_parameters (list): A list of demographic parameters to extract.\n",
    "        categorical_parameters (list): A list of categorical parameters to extract.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the extracted values for demographic parameters,\n",
    "                      categorical parameters, and various statistics for time parameters.\n",
    "    \"\"\"\n",
    "    # Replace -1.0 with NaN\n",
    "    df = df.replace(-1.0, np.nan)\n",
    "    \n",
    "    # Convert \"Time\" column to string\n",
    "    df['Time'] = df['Time'].astype(str)\n",
    "    \n",
    "    # Extract hour from \"Time\" column\n",
    "    df['Hour'] = df['Time'].str.split(':').str[0].astype(int)\n",
    "    \n",
    "    # Pivot the dataframe to have features as columns\n",
    "    df_pivot = df.pivot_table(index='Hour', columns='Parameter', values='Value', aggfunc='median')\n",
    "    \n",
    "    # Create a new dataframe to store the extracted values\n",
    "    new_df = pd.DataFrame()\n",
    "    \n",
    "    # Extract values for demographic parameters\n",
    "    demographic_data = {}\n",
    "    for feature in demographic_parameters:\n",
    "        if feature in df_pivot.columns:\n",
    "            demographic_data[feature] = df_pivot[feature].iloc[0]  # Extract the first value\n",
    "        else:\n",
    "            demographic_data[feature] = np.nan  # Feature not found, assign NaN\n",
    "    demographic_df = pd.DataFrame(demographic_data, index=[0])\n",
    "\n",
    "    # Extract values for categorical parameters\n",
    "    categorical_data = {}\n",
    "    for feature in categorical_parameters:\n",
    "        if feature in df_pivot.columns:\n",
    "            categorical_data[f'{feature}_start'] = df_pivot[df_pivot[feature].notna()].index.min()  # Extract the first value\n",
    "            categorical_data[feature] = df_pivot.loc[df_pivot[df_pivot[feature].notna()].index.min(), feature]\n",
    "        else:\n",
    "            categorical_data[feature] = np.nan  # Feature not found, assign NaN\n",
    "            categorical_data[f'{feature}_start'] = np.nan  # Feature not found, assign NaN\n",
    "    categorical_df = pd.DataFrame(categorical_data, index=[0])\n",
    "\n",
    "    \n",
    "    # Extract median from 0-24, median from 25-48, overal_median, mad, max, slope and last_available value\n",
    "    # for all time parameters\n",
    "    time_data = {}\n",
    "    \n",
    "    for feature in time_parameters_filtered:\n",
    "        if feature in df_pivot.columns:\n",
    "            # Remove outliers for the slope calculation\n",
    "            Q1 = df_pivot[feature].quantile(0.25)\n",
    "            Q3 = df_pivot[feature].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - threshold_for_outliers * IQR\n",
    "            upper_bound = Q3 + threshold_for_outliers * IQR\n",
    "            df_pivot[feature] = df_pivot[feature].where((df_pivot[feature] >= lower_bound) & (df_pivot[feature] <= upper_bound))\n",
    "             \n",
    "            # Calculate median value for 0-24 hours\n",
    "            time_data[f'{feature}_24median'] = df_pivot.loc[0:24, feature].median()\n",
    "            # Calculate median value for 25-48 hours\n",
    "            time_data[f'{feature}_48median'] = df_pivot.loc[25:48, feature].median()\n",
    "            # Calculate overall median value\n",
    "            time_data[f'{feature}_median'] = df_pivot[feature].median()\n",
    "            # Calculate max value\n",
    "            time_data[f'{feature}_max'] = df_pivot[feature].max()\n",
    "            #calculate mad\n",
    "            # Calculate mean absolute deviation (MAD) normalised\n",
    "            # Calculate the absolute differences between each value and the median\n",
    "            absolute_diff_median = (np.abs(df_pivot[feature] - df_pivot[feature].median())).median()\n",
    "            # Calculate the median of these absolute differences\n",
    "            time_data[f'{feature}_mad'] = ((absolute_diff_median/df_pivot[feature].median())*100) if df_pivot[feature].median() != 0 else np.nan\n",
    "            # Calculate last available value\n",
    "            time_data[f'{feature}_last'] = df_pivot[feature].dropna().iloc[-1] if not df_pivot[feature].isnull().all() else np.nan\n",
    "            # Fit linear regression to the data\n",
    "            # Flatten the data for the feature across all hours\n",
    "            feature_values = df_pivot[feature].dropna().values\n",
    "            all_hours = df_pivot[feature].dropna().index.values\n",
    "            \n",
    "            if len(all_hours) > 1:\n",
    "                # Fit a linear regression line to the data\n",
    "                coefficients = np.polyfit(all_hours, feature_values, 1)\n",
    "                # Store the slope in the dataframe\n",
    "                time_data[f'{feature}_slope'] = coefficients[0]\n",
    "            else:\n",
    "                # Insufficient data points for linear regression, assign NaN\n",
    "                time_data[f'{feature}_slope'] = np.nan\n",
    "           \n",
    "        else:\n",
    "             # Calculate median value for 0-24 hours\n",
    "            time_data[f'{feature}_24median'] = np.nan\n",
    "            time_data[f'{feature}_48median'] = np.nan\n",
    "            time_data[f'{feature}_median'] = np.nan\n",
    "            time_data[f'{feature}_max'] = np.nan\n",
    "            time_data[f'{feature}_mad'] = np.nan\n",
    "            time_data[f'{feature}_last'] = np.nan\n",
    "            time_data[f'{feature}_slope'] = np.nan\n",
    "\n",
    "    time_df = pd.DataFrame(time_data, index=[0])\n",
    " \n",
    "    \n",
    "    # Concatenate all dataframes along columns axis\n",
    "    final_df = pd.concat([demographic_df, categorical_df, time_df], axis=1)\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "#Uncomment this to create file!!\n",
    "#save dataframe for feature cleaning\n",
    "#result = process_time_series_files(time_series_files, time_parameters, demographic_parameters, categorical_parameters, extract_summarysed_features)\n",
    "#result.to_csv(os.path.join(file_path_to_save, \"uncleaned_summarysed_features.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "558620ea-35a2-44e3-8486-7b771b5726c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns after feature extraction: 259\n"
     ]
    }
   ],
   "source": [
    "final_dataframe = pd.read_csv(os.path.join(file_path_to_save, 'uncleaned_summarysed_features.csv'))\n",
    "print(\"Number of columns after feature extraction:\", final_dataframe.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "7f3a1e24-d8e3-452f-9111-5b3edbe7e7dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RecordID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>ICUType</th>\n",
       "      <th>BMI</th>\n",
       "      <th>MechVent</th>\n",
       "      <th>MechVent_start</th>\n",
       "      <th>Albumin_24median</th>\n",
       "      <th>Albumin_48median</th>\n",
       "      <th>Albumin_median</th>\n",
       "      <th>...</th>\n",
       "      <th>WBC_max</th>\n",
       "      <th>WBC_mad</th>\n",
       "      <th>WBC_last</th>\n",
       "      <th>WBC_slope</th>\n",
       "      <th>Weight_24median</th>\n",
       "      <th>Weight_48median</th>\n",
       "      <th>Weight_max</th>\n",
       "      <th>Weight_mad</th>\n",
       "      <th>Weight_last</th>\n",
       "      <th>Weight_slope</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>132539.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>11.2</td>\n",
       "      <td>8.737864</td>\n",
       "      <td>9.4</td>\n",
       "      <td>-0.078261</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>132540.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>26.228364</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>13.3</td>\n",
       "      <td>1.526718</td>\n",
       "      <td>13.3</td>\n",
       "      <td>0.126301</td>\n",
       "      <td>80.6</td>\n",
       "      <td>80.6</td>\n",
       "      <td>80.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>80.6</td>\n",
       "      <td>5.728184e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>132541.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.5</td>\n",
       "      <td>...</td>\n",
       "      <td>6.2</td>\n",
       "      <td>11.904762</td>\n",
       "      <td>6.2</td>\n",
       "      <td>0.058981</td>\n",
       "      <td>56.7</td>\n",
       "      <td>56.7</td>\n",
       "      <td>56.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>56.7</td>\n",
       "      <td>-7.200356e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>132543.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>26.024291</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.4</td>\n",
       "      <td>...</td>\n",
       "      <td>11.5</td>\n",
       "      <td>10.227273</td>\n",
       "      <td>7.9</td>\n",
       "      <td>-0.095192</td>\n",
       "      <td>84.6</td>\n",
       "      <td>84.6</td>\n",
       "      <td>84.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>84.6</td>\n",
       "      <td>-5.533244e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>132545.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.3</td>\n",
       "      <td>3.3</td>\n",
       "      <td>...</td>\n",
       "      <td>4.8</td>\n",
       "      <td>11.627907</td>\n",
       "      <td>4.8</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 258 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   RecordID   Age  Gender  ICUType        BMI  MechVent  MechVent_start  \\\n",
       "0  132539.0  54.0     0.0      4.0        NaN       0.0             NaN   \n",
       "1  132540.0  76.0     1.0      2.0  26.228364       1.0             1.0   \n",
       "2  132541.0  44.0     0.0      3.0        NaN       1.0            10.0   \n",
       "3  132543.0  68.0     1.0      3.0  26.024291       0.0             NaN   \n",
       "4  132545.0  88.0     0.0      3.0        NaN       0.0             NaN   \n",
       "\n",
       "   Albumin_24median  Albumin_48median  Albumin_median  ...  WBC_max  \\\n",
       "0               NaN               NaN             NaN  ...     11.2   \n",
       "1               NaN               NaN             NaN  ...     13.3   \n",
       "2               2.5               NaN             2.5  ...      6.2   \n",
       "3               4.4               NaN             4.4  ...     11.5   \n",
       "4               NaN               3.3             3.3  ...      4.8   \n",
       "\n",
       "     WBC_mad  WBC_last  WBC_slope  Weight_24median  Weight_48median  \\\n",
       "0   8.737864       9.4  -0.078261              NaN              NaN   \n",
       "1   1.526718      13.3   0.126301             80.6             80.6   \n",
       "2  11.904762       6.2   0.058981             56.7             56.7   \n",
       "3  10.227273       7.9  -0.095192             84.6             84.6   \n",
       "4  11.627907       4.8   0.038462              NaN              NaN   \n",
       "\n",
       "   Weight_max  Weight_mad  Weight_last  Weight_slope  \n",
       "0         NaN         NaN          NaN           NaN  \n",
       "1        80.6         0.0         80.6  5.728184e-16  \n",
       "2        56.7         0.0         56.7 -7.200356e-16  \n",
       "3        84.6         0.0         84.6 -5.533244e-16  \n",
       "4         NaN         NaN          NaN           NaN  \n",
       "\n",
       "[5 rows x 258 columns]"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#impute Mech_vent Nan values with 0\n",
    "final_dataframe['MechVent'] = final_dataframe['MechVent'].fillna(0)\n",
    "\n",
    "#Create BMI value and drop 'Height', 'Weight_median'\n",
    "final_dataframe['BMI'] = np.where(final_dataframe['Height'].isnull() | final_dataframe['Weight_median'].isnull(), np.nan, (final_dataframe['Weight_median'] / ((final_dataframe['Height'] / 100) ** 2)))\n",
    "                               \n",
    "#Add BMI to my demo features\n",
    "demographic_parameters.append('BMI')\n",
    "demographic_parameters.remove('Height')\n",
    "#Reorder dataframe so that demo parameters go first \n",
    "\n",
    "# Get a list of all column names\n",
    "columns = list(final_dataframe.columns)\n",
    "\n",
    "# Move demographic parameters to the beginning of the list\n",
    "for param in demographic_parameters[::-1]:\n",
    "    columns.remove(param)\n",
    "    columns.insert(0, param)\n",
    "\n",
    "# Reorder the DataFrame columns\n",
    "final_dataframe = final_dataframe[columns]\n",
    "final_dataframe.drop(columns=['Height', 'Weight_median'], inplace=True)   \n",
    "final_dataframe.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd4e534-9e83-496f-9c90-aa15c335779a",
   "metadata": {},
   "source": [
    "### 2. Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb032b52-6d9a-4b96-b7e1-461648f09145",
   "metadata": {},
   "source": [
    "##### Removing features with more than 50% NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "61efc3f4-d099-48c4-80cd-7e6bc17ea761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features missing more than 50% of values: 73\n",
      "Name fo the features missing more than 50% of values: ['TropI_max', 'TropI_mad', 'TropI_last', 'TropI_slope', 'TropT_24median', 'TropT_48median', 'TropI_median', 'TropI_48median', 'TropI_24median', 'TropT_median', 'TropT_max', 'TropT_mad', 'TropT_last', 'TropT_slope', 'Cholesterol_slope', 'Cholesterol_48median', 'Cholesterol_24median', 'Cholesterol_max', 'Cholesterol_median', 'Cholesterol_last', 'Cholesterol_mad', 'Albumin_slope', 'Albumin_48median', 'ALP_slope', 'AST_slope', 'ALT_slope', 'Bilirubin_slope', 'ALP_48median', 'AST_48median', 'ALT_48median', 'Bilirubin_48median', 'Lactate_48median', 'SaO2_48median', 'RespRate_48median', 'RespRate_24median', 'RespRate_slope', 'RespRate_last', 'RespRate_mad', 'RespRate_median', 'RespRate_max', 'SaO2_slope', 'Albumin_24median', 'Lactate_slope', 'ALP_24median', 'Bilirubin_24median', 'SaO2_24median', 'ALT_24median', 'AST_24median', 'Albumin_mad', 'Albumin_median', 'Albumin_max', 'Albumin_last', 'ALP_mad', 'ALP_max', 'ALP_median', 'ALP_last', 'Bilirubin_mad', 'Bilirubin_median', 'Bilirubin_max', 'Bilirubin_last', 'ALT_last', 'ALT_median', 'ALT_max', 'ALT_mad', 'AST_median', 'AST_max', 'AST_mad', 'AST_last', 'SaO2_mad', 'SaO2_last', 'SaO2_max', 'SaO2_median', 'Lactate_24median']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Missing Percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>TropI_max</td>\n",
       "      <td>100.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>TropI_mad</td>\n",
       "      <td>100.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>TropI_last</td>\n",
       "      <td>100.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>TropI_slope</td>\n",
       "      <td>100.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>TropT_24median</td>\n",
       "      <td>100.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>SaO2_mad</td>\n",
       "      <td>55.6875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>SaO2_last</td>\n",
       "      <td>55.6875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>SaO2_max</td>\n",
       "      <td>55.6875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>SaO2_median</td>\n",
       "      <td>55.6875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>Lactate_24median</td>\n",
       "      <td>50.4500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>73 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Feature  Missing Percentage\n",
       "227         TropI_max            100.0000\n",
       "228         TropI_mad            100.0000\n",
       "229        TropI_last            100.0000\n",
       "230       TropI_slope            100.0000\n",
       "231    TropT_24median            100.0000\n",
       "..                ...                 ...\n",
       "207          SaO2_mad             55.6875\n",
       "208         SaO2_last             55.6875\n",
       "206          SaO2_max             55.6875\n",
       "205       SaO2_median             55.6875\n",
       "119  Lactate_24median             50.4500\n",
       "\n",
       "[73 rows x 2 columns]"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate percentage of missing values per feature\n",
    "total_rows = len(final_dataframe)\n",
    "missing_values_percentage = (final_dataframe.isnull().sum() / total_rows) * 100\n",
    "\n",
    "# Create a DataFrame to store missing values percentage\n",
    "missing_percentage_table = pd.DataFrame({\n",
    "    'Feature': missing_values_percentage.index,\n",
    "    'Missing Percentage': missing_values_percentage.values\n",
    "})\n",
    "\n",
    "# Sort missing percentage table by Missing Percentage column in descending order\n",
    "missing_percentage_table_sorted = missing_percentage_table.sort_values(by='Missing Percentage', ascending=False)\n",
    "\n",
    "# list features with missing values >50%\n",
    "features_missing_gt_50 = missing_percentage_table_sorted[missing_percentage_table_sorted['Missing Percentage'] >= thresholds_for_non_important_features]\n",
    "print(\"Number of features missing more than 50% of values:\", features_missing_gt_50.shape[0])\n",
    "# List the feature names\n",
    "print(\"Name fo the features missing more than 50% of values:\", features_missing_gt_50['Feature'].tolist())\n",
    "features_missing_gt_50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "2b32b6d5-f389-4b6d-8c8e-b2a9596b2f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features after eliminating >50% missing features: 185\n"
     ]
    }
   ],
   "source": [
    "# Create a boolean mask for features with missing percentage >= 70\n",
    "mask = missing_values_percentage >= thresholds_for_non_important_features \n",
    "final_dataframe_filtered = final_dataframe.loc[:, ~mask]\n",
    "print(\"Number of features after eliminating >50% missing features:\", final_dataframe_filtered.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd5faa1-7372-4ffd-b0d7-783aad5e3bf3",
   "metadata": {},
   "source": [
    "#### Removing low variance features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "67196b3a-a2a1-402b-992d-4eafb8d6e56e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected columns: Index(['MechVent_start', 'BUN_24median', 'BUN_48median', 'BUN_median',\n",
      "       'BUN_max', 'BUN_mad', 'BUN_last', 'BUN_slope', 'Creatinine_24median',\n",
      "       'Creatinine_48median',\n",
      "       ...\n",
      "       'WBC_median', 'WBC_max', 'WBC_mad', 'WBC_last', 'WBC_slope',\n",
      "       'Weight_24median', 'Weight_48median', 'Weight_max', 'Weight_mad',\n",
      "       'Weight_last'],\n",
      "      dtype='object', length=178)\n",
      "Dropped columns: Index(['Weight_slope'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "X = final_dataframe_filtered.drop(columns=demographic_parameters + categorical_parameters)\n",
    "\n",
    "# Step 1: Scale your features using RobustScaler\n",
    "scaler = RobustScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Instantiate the VarianceThreshold object with the desired threshold\n",
    "# Step 2: Use VarianceThreshold to select features based on variance\n",
    "threshold = 0.05  # Set your desired threshold\n",
    "sel = VarianceThreshold(threshold=threshold)\n",
    "X_selection = sel.fit_transform(X_scaled)\n",
    "\n",
    "# Get boolean mask of selected features\n",
    "selected_columns_mask = sel.get_support()\n",
    "\n",
    "# Get selected column names\n",
    "selected_columns = X.columns[selected_columns_mask]\n",
    "\n",
    "# Get dropped column names\n",
    "dropped_columns = X.columns[~selected_columns_mask]\n",
    "\n",
    "# Print selected and dropped column names\n",
    "print(\"Selected columns:\", selected_columns)\n",
    "print(\"Dropped columns:\", dropped_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "a4e9f251-d47f-4586-adad-ebdd56435447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns after low variance dropped: 184\n"
     ]
    }
   ],
   "source": [
    "final_dataframe_filtered = final_dataframe_filtered.drop(columns=dropped_columns.tolist())\n",
    "print(\"Number of columns after low variance dropped:\", final_dataframe_filtered.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a739c95-77fa-4446-a29a-8e3057fa1efc",
   "metadata": {},
   "source": [
    "#### Removing high correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "8549386a-d6ed-4897-b5b5-f7b991cecbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_4correlation = final_dataframe_filtered.drop(columns=demographic_parameters + categorical_parameters)\n",
    "\n",
    "#Re-order the dataframe so that we prioritize keeping the median features just because it is easy to interprete later\n",
    "median_cols = [col for col in dataframe_4correlation.columns if col.endswith(\"_median\")]\n",
    "non_median_cols = [col for col in dataframe_4correlation.columns if not col.endswith(\"_median\")]\n",
    "ordered_cols = median_cols + non_median_cols\n",
    "\n",
    "# Create the reordered DataFrame\n",
    "dataframe_4correlation_reordered = dataframe_4correlation[ordered_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "ae8a204f-f4ed-4ac4-a61b-0884bbd4f8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation matrix\n",
    "\n",
    "corr_matrix = dataframe_4correlation_reordered.corr().abs()\n",
    "# Set upper triangle of correlation matrix to NaN\n",
    "corr_matrix.values[np.triu_indices_from(corr_matrix)] = np.nan\n",
    "\n",
    "# Find highly correlated features\n",
    "highly_correlated_features = set()\n",
    "for col in corr_matrix.columns:\n",
    "    correlated_cols = corr_matrix[col][corr_matrix[col] > 0.8].index.tolist()\n",
    "    #print(col, \",\", correlated_cols)\n",
    "    # Drop the first column in each pair of highly correlated features\n",
    "    highly_correlated_features.update(correlated_cols)\n",
    "\n",
    "final_dataframe = final_dataframe_filtered.drop(columns=highly_correlated_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "071e4178-588a-487a-a93f-0552a3dffc23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature 1</th>\n",
       "      <th>Feature 2</th>\n",
       "      <th>Correlation Coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Feature 1, Feature 2, Correlation Coefficient]\n",
       "Index: []"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# oduble checking we haven't missed dropping any columns\n",
    "corr_matrix = final_dataframe.corr().abs()\n",
    "threshold = 0.8  # Choose a threshold\n",
    "\n",
    "# find correlations\n",
    "high_correlations = find_high_correlations(corr_matrix, threshold)\n",
    "\n",
    "# Convert the list to a DataFrame for easier manipulation\n",
    "high_correlations_df = pd.DataFrame(high_correlations, columns=['Feature 1', 'Feature 2', 'Correlation Coefficient'])\n",
    "\n",
    "# Display the DataFrame\n",
    "high_correlations_df.to_csv(os.path.join(file_path_to_save, \"high_correlated_variables.csv\"), index=False)\n",
    "high_correlations_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "d6aabcee-0ed1-4494-a41d-4d97bf2f80b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns after correlation features dropped: 99\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of columns after correlation features dropped:\", final_dataframe.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "349be024",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save dataframe \n",
    "final_dataframe.to_csv(os.path.join(file_path_to_save, \"clean_dataframe_with_NaN.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ffbc4b",
   "metadata": {},
   "source": [
    "Now we have our final df saved and ready for next step : NaN imputation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
