{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GLOBAL VARIABLES\n",
    "# Define the programmer whose path you want to use\n",
    "programmer = 'patricia'\n",
    "\n",
    "#variables for paths names\n",
    "file_paths_names = {\n",
    "    'patricia': r\"..\\original_data\",\n",
    "    'esther': r\"C:\\Users\\egh22\\OneDrive - University of Canterbury\\bootcamp\\group project\\data\"\n",
    "}\n",
    "\n",
    "files_outcome_path_names = {\n",
    "    'patricia': r\"..\\original_data\\outcomes\",\n",
    "    'esther': r\"C:\\Users\\egh22\\OneDrive - University of Canterbury\\bootcamp\\group project\\data\\outcomes\"\n",
    "}\n",
    "\n",
    "\n",
    "files_path_to_save_files = {\n",
    "    'patricia': r\"..\\clean_data\",\n",
    "    'esther': r\"C:\\Users\\egh22\\OneDrive - University of Canterbury\\bootcamp\\group project\\clean_data\"\n",
    "}\n",
    "\n",
    "# Choose the file path based on the programmer variable\n",
    "file_path_to_save = files_path_to_save_files[programmer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> 1 - Data importation </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features importation\n",
    "initial_df = pd.read_csv(os.path.join(file_path_to_save, 'clean_dataframe_with_NaN.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#outcomes importation\n",
    "files_outcome_path =  files_outcome_path_names[programmer]\n",
    "df_target_a = pd.read_csv(os.path.join(files_outcome_path, \"Outcomes-a.txt\"))\n",
    "df_target_b = pd.read_csv(os.path.join(files_outcome_path, \"Outcomes-b.txt\"))\n",
    "\n",
    "outcomes_df = pd.concat([df_target_a, df_target_b], ignore_index=True).drop(columns=['SAPS-I', 'SOFA', 'Survival'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> METHOD 1 - KNNImputer (mputation for completing missing values using k-Nearest Neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize KNNImputer\n",
    "imputer = KNNImputer(n_neighbors=200)\n",
    "\n",
    "# Impute missing values in initial_df\n",
    "imputed_data = imputer.fit_transform(initial_df)\n",
    "\n",
    "# Convert the result back to a DataFrame\n",
    "KNN_imputed_df = pd.DataFrame(imputed_data, columns=initial_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RecordID         0\n",
       "Age              0\n",
       "Gender           0\n",
       "Height           0\n",
       "ICUType          0\n",
       "                ..\n",
       "WBC_mad          0\n",
       "WBC_slope        0\n",
       "Weight_median    0\n",
       "Weight_mad       0\n",
       "Weight_slope     0\n",
       "Length: 119, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sannity check\n",
    "KNN_imputed_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "merge the dataframe with the outcoms for machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the imputed dataframe with the outcomes on each patient ID\n",
    "\n",
    "KNN_imputed_df = pd.merge(KNN_imputed_df, outcomes_df, on='RecordID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the new dataframe\n",
    "KNN_imputed_df.to_csv(os.path.join(file_path_to_save, \"KNN_imputed_df.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "variables of KNNImputer:\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.impute.KNNImputer.html\n",
    "Parameters:\n",
    "missing_valuesint, float, str, np.nan or None, default=np.nan\n",
    "The placeholder for the missing values. All occurrences of missing_values will be imputed. For pandas’ dataframes with nullable integer dtypes with missing values, missing_values should be set to np.nan, since pd.NA will be converted to np.nan.\n",
    "\n",
    "n_neighborsint, default=5\n",
    "Number of neighboring samples to use for imputation.\n",
    "\n",
    "weights{‘uniform’, ‘distance’} or callable, default=’uniform’\n",
    "Weight function used in prediction. Possible values:\n",
    "\n",
    "‘uniform’ : uniform weights. All points in each neighborhood are weighted equally.\n",
    "\n",
    "‘distance’ : weight points by the inverse of their distance. in this case, closer neighbors of a query point will have a greater influence than neighbors which are further away.\n",
    "\n",
    "callable : a user-defined function which accepts an array of distances, and returns an array of the same shape containing the weights.\n",
    "\n",
    "metric{‘nan_euclidean’} or callable, default=’nan_euclidean’\n",
    "Distance metric for searching neighbors. Possible values:\n",
    "\n",
    "‘nan_euclidean’\n",
    "\n",
    "callable : a user-defined function which conforms to the definition of _pairwise_callable(X, Y, metric, **kwds). The function accepts two arrays, X and Y, and a missing_values keyword in kwds and returns a scalar distance value.\n",
    "\n",
    "copybool, default=True\n",
    "If True, a copy of X will be created. If False, imputation will be done in-place whenever possible.\n",
    "\n",
    "add_indicatorbool, default=False\n",
    "If True, a MissingIndicator transform will stack onto the output of the imputer’s transform. This allows a predictive estimator to account for missingness despite imputation. If a feature has no missing values at fit/train time, the feature won’t appear on the missing indicator even if there are missing values at transform/test time.\n",
    "\n",
    "keep_empty_featuresbool, default=False\n",
    "If True, features that consist exclusively of missing values when fit is called are returned in results when transform is called. The imputed value is always 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> METHOD 2 - PCA (Principal Comopnent Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ironhack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
