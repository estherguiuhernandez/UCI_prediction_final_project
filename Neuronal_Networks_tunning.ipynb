{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\psaez\\miniconda3\\envs\\ironhack\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, cohen_kappa_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, BatchNormalization, Dropout\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import make_scorer\n",
    "import tensorflow as tf\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPTIMIZATION OF THE NEURAL NETWORK MODEL\n",
    "\n",
    "After the first evaluation on In_hospital_death the m1_df with RandomOverSampler was he one with less deviation and slightly better recall. Thus the optimization will be done using m1_df and the same balancing technique. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1_df = pd.read_csv('clean_data/withsetc/minus1_imputed_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2- Optimization process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1 -Create grids to evaluate different parameters**\n",
    "\n",
    "in order to increase speed in a firs step we will evaluate:\n",
    " -  'optimizer': ['adam', 'rmsprop', tf.keras.optimizers.legacy.SGD]\n",
    " -  'hidden_layers': [1, 2, 3, 5]\n",
    " -  'units': [32, 64, 128]\n",
    " -  'batch_size': [16, 32, 64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to create the model\n",
    "def create_model(optimizer='sgd', hidden_layers=1, units=64, batch_size=32):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    for _ in range(hidden_layers):\n",
    "        model.add(Dense(units, activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features and target variable\n",
    "X = m1_df.drop(columns=['In-hospital_death', 'Length_of_stay', 'RecordID'])\n",
    "y = m1_df['In-hospital_death']\n",
    "\n",
    "# Data splitting\n",
    "test_size = 0.2\n",
    "random_state = 42\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "# Data balancing\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_train, y_train = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "# Data normalization\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 108 candidates, totalling 324 fits\n",
      "WARNING:tensorflow:From c:\\Users\\psaez\\miniconda3\\envs\\ironhack\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\psaez\\miniconda3\\envs\\ironhack\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\psaez\\miniconda3\\envs\\ironhack\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\psaez\\miniconda3\\envs\\ironhack\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "[CV] END batch_size=16, hidden_layers=1, optimizer=adam, units=32; total time=   1.9s\n",
      "[CV] END batch_size=16, hidden_layers=1, optimizer=adam, units=32; total time=   1.3s\n",
      "[CV] END batch_size=16, hidden_layers=1, optimizer=adam, units=32; total time=   1.4s\n",
      "[CV] END batch_size=16, hidden_layers=1, optimizer=adam, units=64; total time=   1.3s\n",
      "[CV] END batch_size=16, hidden_layers=1, optimizer=adam, units=64; total time=   1.3s\n",
      "[CV] END batch_size=16, hidden_layers=1, optimizer=adam, units=64; total time=   1.3s\n",
      "[CV] END batch_size=16, hidden_layers=1, optimizer=adam, units=128; total time=   1.3s\n",
      "[CV] END batch_size=16, hidden_layers=1, optimizer=adam, units=128; total time=   1.3s\n",
      "[CV] END batch_size=16, hidden_layers=1, optimizer=adam, units=128; total time=   1.3s\n",
      "[CV] END batch_size=16, hidden_layers=1, optimizer=rmsprop, units=32; total time=   1.3s\n",
      "[CV] END batch_size=16, hidden_layers=1, optimizer=rmsprop, units=32; total time=   1.4s\n",
      "[CV] END batch_size=16, hidden_layers=1, optimizer=rmsprop, units=32; total time=   1.3s\n",
      "[CV] END batch_size=16, hidden_layers=1, optimizer=rmsprop, units=64; total time=   1.3s\n",
      "[CV] END batch_size=16, hidden_layers=1, optimizer=rmsprop, units=64; total time=   1.3s\n",
      "[CV] END batch_size=16, hidden_layers=1, optimizer=rmsprop, units=64; total time=   1.3s\n",
      "[CV] END batch_size=16, hidden_layers=1, optimizer=rmsprop, units=128; total time=   1.4s\n",
      "[CV] END batch_size=16, hidden_layers=1, optimizer=rmsprop, units=128; total time=   1.3s\n",
      "[CV] END batch_size=16, hidden_layers=1, optimizer=rmsprop, units=128; total time=   1.3s\n",
      "[CV] END batch_size=16, hidden_layers=1, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=32; total time=   1.3s\n",
      "[CV] END batch_size=16, hidden_layers=1, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=32; total time=   1.3s\n",
      "[CV] END batch_size=16, hidden_layers=1, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=32; total time=   1.3s\n",
      "[CV] END batch_size=16, hidden_layers=1, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=64; total time=   1.3s\n",
      "[CV] END batch_size=16, hidden_layers=1, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=64; total time=   1.5s\n",
      "[CV] END batch_size=16, hidden_layers=1, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=64; total time=   1.3s\n",
      "[CV] END batch_size=16, hidden_layers=1, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=128; total time=   1.4s\n",
      "[CV] END batch_size=16, hidden_layers=1, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=128; total time=   1.4s\n",
      "[CV] END batch_size=16, hidden_layers=1, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=128; total time=   1.4s\n",
      "[CV] END batch_size=16, hidden_layers=2, optimizer=adam, units=32; total time=   1.5s\n",
      "[CV] END batch_size=16, hidden_layers=2, optimizer=adam, units=32; total time=   1.6s\n",
      "[CV] END batch_size=16, hidden_layers=2, optimizer=adam, units=32; total time=   1.5s\n",
      "[CV] END batch_size=16, hidden_layers=2, optimizer=adam, units=64; total time=   1.6s\n",
      "[CV] END batch_size=16, hidden_layers=2, optimizer=adam, units=64; total time=   1.6s\n",
      "[CV] END batch_size=16, hidden_layers=2, optimizer=adam, units=64; total time=   1.6s\n",
      "[CV] END batch_size=16, hidden_layers=2, optimizer=adam, units=128; total time=   1.6s\n",
      "[CV] END batch_size=16, hidden_layers=2, optimizer=adam, units=128; total time=   1.9s\n",
      "[CV] END batch_size=16, hidden_layers=2, optimizer=adam, units=128; total time=   1.7s\n",
      "[CV] END batch_size=16, hidden_layers=2, optimizer=rmsprop, units=32; total time=   1.6s\n",
      "[CV] END batch_size=16, hidden_layers=2, optimizer=rmsprop, units=32; total time=   1.6s\n",
      "[CV] END batch_size=16, hidden_layers=2, optimizer=rmsprop, units=32; total time=   1.6s\n",
      "[CV] END batch_size=16, hidden_layers=2, optimizer=rmsprop, units=64; total time=   1.5s\n",
      "[CV] END batch_size=16, hidden_layers=2, optimizer=rmsprop, units=64; total time=   1.6s\n",
      "[CV] END batch_size=16, hidden_layers=2, optimizer=rmsprop, units=64; total time=   1.6s\n",
      "[CV] END batch_size=16, hidden_layers=2, optimizer=rmsprop, units=128; total time=   1.6s\n",
      "[CV] END batch_size=16, hidden_layers=2, optimizer=rmsprop, units=128; total time=   1.6s\n",
      "[CV] END batch_size=16, hidden_layers=2, optimizer=rmsprop, units=128; total time=   1.6s\n",
      "[CV] END batch_size=16, hidden_layers=2, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=32; total time=   1.6s\n",
      "[CV] END batch_size=16, hidden_layers=2, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=32; total time=   1.6s\n",
      "[CV] END batch_size=16, hidden_layers=2, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=32; total time=   1.5s\n",
      "[CV] END batch_size=16, hidden_layers=2, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=64; total time=   1.8s\n",
      "[CV] END batch_size=16, hidden_layers=2, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=64; total time=   1.6s\n",
      "[CV] END batch_size=16, hidden_layers=2, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=64; total time=   1.6s\n",
      "[CV] END batch_size=16, hidden_layers=2, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=128; total time=   1.6s\n",
      "[CV] END batch_size=16, hidden_layers=2, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=128; total time=   1.6s\n",
      "[CV] END batch_size=16, hidden_layers=2, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=128; total time=   1.6s\n",
      "[CV] END batch_size=16, hidden_layers=3, optimizer=adam, units=32; total time=   1.8s\n",
      "[CV] END batch_size=16, hidden_layers=3, optimizer=adam, units=32; total time=   1.8s\n",
      "[CV] END batch_size=16, hidden_layers=3, optimizer=adam, units=32; total time=   1.8s\n",
      "[CV] END batch_size=16, hidden_layers=3, optimizer=adam, units=64; total time=   1.8s\n",
      "[CV] END batch_size=16, hidden_layers=3, optimizer=adam, units=64; total time=   1.8s\n",
      "[CV] END batch_size=16, hidden_layers=3, optimizer=adam, units=64; total time=   1.8s\n",
      "[CV] END batch_size=16, hidden_layers=3, optimizer=adam, units=128; total time=   1.9s\n",
      "[CV] END batch_size=16, hidden_layers=3, optimizer=adam, units=128; total time=   1.8s\n",
      "[CV] END batch_size=16, hidden_layers=3, optimizer=adam, units=128; total time=   2.2s\n",
      "[CV] END batch_size=16, hidden_layers=3, optimizer=rmsprop, units=32; total time=   1.8s\n",
      "[CV] END batch_size=16, hidden_layers=3, optimizer=rmsprop, units=32; total time=   1.8s\n",
      "[CV] END batch_size=16, hidden_layers=3, optimizer=rmsprop, units=32; total time=   1.8s\n",
      "[CV] END batch_size=16, hidden_layers=3, optimizer=rmsprop, units=64; total time=   1.8s\n",
      "[CV] END batch_size=16, hidden_layers=3, optimizer=rmsprop, units=64; total time=   1.8s\n",
      "[CV] END batch_size=16, hidden_layers=3, optimizer=rmsprop, units=64; total time=   1.8s\n",
      "[CV] END batch_size=16, hidden_layers=3, optimizer=rmsprop, units=128; total time=   1.9s\n",
      "[CV] END batch_size=16, hidden_layers=3, optimizer=rmsprop, units=128; total time=   1.9s\n",
      "[CV] END batch_size=16, hidden_layers=3, optimizer=rmsprop, units=128; total time=   1.9s\n",
      "[CV] END batch_size=16, hidden_layers=3, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=32; total time=   1.8s\n",
      "[CV] END batch_size=16, hidden_layers=3, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=32; total time=   1.8s\n",
      "[CV] END batch_size=16, hidden_layers=3, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=32; total time=   1.8s\n",
      "[CV] END batch_size=16, hidden_layers=3, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=64; total time=   1.8s\n",
      "[CV] END batch_size=16, hidden_layers=3, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=64; total time=   1.9s\n",
      "[CV] END batch_size=16, hidden_layers=3, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=64; total time=   2.2s\n",
      "[CV] END batch_size=16, hidden_layers=3, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=128; total time=   1.9s\n",
      "[CV] END batch_size=16, hidden_layers=3, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=128; total time=   1.9s\n",
      "[CV] END batch_size=16, hidden_layers=3, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=128; total time=   1.9s\n",
      "[CV] END batch_size=16, hidden_layers=5, optimizer=adam, units=32; total time=   2.2s\n",
      "[CV] END batch_size=16, hidden_layers=5, optimizer=adam, units=32; total time=   2.3s\n",
      "[CV] END batch_size=16, hidden_layers=5, optimizer=adam, units=32; total time=   2.3s\n",
      "[CV] END batch_size=16, hidden_layers=5, optimizer=adam, units=64; total time=   2.2s\n",
      "[CV] END batch_size=16, hidden_layers=5, optimizer=adam, units=64; total time=   2.3s\n",
      "[CV] END batch_size=16, hidden_layers=5, optimizer=adam, units=64; total time=   2.3s\n",
      "[CV] END batch_size=16, hidden_layers=5, optimizer=adam, units=128; total time=   2.4s\n",
      "[CV] END batch_size=16, hidden_layers=5, optimizer=adam, units=128; total time=   2.4s\n",
      "[CV] END batch_size=16, hidden_layers=5, optimizer=adam, units=128; total time=   2.4s\n",
      "[CV] END batch_size=16, hidden_layers=5, optimizer=rmsprop, units=32; total time=   2.3s\n",
      "[CV] END batch_size=16, hidden_layers=5, optimizer=rmsprop, units=32; total time=   2.3s\n",
      "[CV] END batch_size=16, hidden_layers=5, optimizer=rmsprop, units=32; total time=   2.8s\n",
      "[CV] END batch_size=16, hidden_layers=5, optimizer=rmsprop, units=64; total time=   2.4s\n",
      "[CV] END batch_size=16, hidden_layers=5, optimizer=rmsprop, units=64; total time=   2.3s\n",
      "[CV] END batch_size=16, hidden_layers=5, optimizer=rmsprop, units=64; total time=   2.4s\n",
      "[CV] END batch_size=16, hidden_layers=5, optimizer=rmsprop, units=128; total time=   2.4s\n",
      "[CV] END batch_size=16, hidden_layers=5, optimizer=rmsprop, units=128; total time=   2.4s\n",
      "[CV] END batch_size=16, hidden_layers=5, optimizer=rmsprop, units=128; total time=   2.4s\n",
      "[CV] END batch_size=16, hidden_layers=5, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=32; total time=   2.3s\n",
      "[CV] END batch_size=16, hidden_layers=5, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=32; total time=   2.3s\n",
      "[CV] END batch_size=16, hidden_layers=5, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=32; total time=   2.3s\n",
      "[CV] END batch_size=16, hidden_layers=5, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=64; total time=   2.3s\n",
      "[CV] END batch_size=16, hidden_layers=5, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=64; total time=   2.2s\n",
      "[CV] END batch_size=16, hidden_layers=5, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=64; total time=   2.3s\n",
      "[CV] END batch_size=16, hidden_layers=5, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=128; total time=   2.4s\n",
      "[CV] END batch_size=16, hidden_layers=5, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=128; total time=   2.4s\n",
      "[CV] END batch_size=16, hidden_layers=5, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=128; total time=   3.1s\n",
      "[CV] END batch_size=32, hidden_layers=1, optimizer=adam, units=32; total time=   0.9s\n",
      "[CV] END batch_size=32, hidden_layers=1, optimizer=adam, units=32; total time=   1.0s\n",
      "[CV] END batch_size=32, hidden_layers=1, optimizer=adam, units=32; total time=   1.0s\n",
      "[CV] END batch_size=32, hidden_layers=1, optimizer=adam, units=64; total time=   1.0s\n",
      "[CV] END batch_size=32, hidden_layers=1, optimizer=adam, units=64; total time=   1.0s\n",
      "[CV] END batch_size=32, hidden_layers=1, optimizer=adam, units=64; total time=   1.0s\n",
      "[CV] END batch_size=32, hidden_layers=1, optimizer=adam, units=128; total time=   1.0s\n",
      "[CV] END batch_size=32, hidden_layers=1, optimizer=adam, units=128; total time=   1.0s\n",
      "[CV] END batch_size=32, hidden_layers=1, optimizer=adam, units=128; total time=   1.0s\n",
      "[CV] END batch_size=32, hidden_layers=1, optimizer=rmsprop, units=32; total time=   0.9s\n",
      "[CV] END batch_size=32, hidden_layers=1, optimizer=rmsprop, units=32; total time=   1.0s\n",
      "[CV] END batch_size=32, hidden_layers=1, optimizer=rmsprop, units=32; total time=   1.0s\n",
      "[CV] END batch_size=32, hidden_layers=1, optimizer=rmsprop, units=64; total time=   1.0s\n",
      "[CV] END batch_size=32, hidden_layers=1, optimizer=rmsprop, units=64; total time=   1.0s\n",
      "[CV] END batch_size=32, hidden_layers=1, optimizer=rmsprop, units=64; total time=   1.0s\n",
      "[CV] END batch_size=32, hidden_layers=1, optimizer=rmsprop, units=128; total time=   1.0s\n",
      "[CV] END batch_size=32, hidden_layers=1, optimizer=rmsprop, units=128; total time=   1.0s\n",
      "[CV] END batch_size=32, hidden_layers=1, optimizer=rmsprop, units=128; total time=   1.0s\n",
      "[CV] END batch_size=32, hidden_layers=1, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=32; total time=   0.9s\n",
      "[CV] END batch_size=32, hidden_layers=1, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=32; total time=   0.9s\n",
      "[CV] END batch_size=32, hidden_layers=1, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=32; total time=   0.9s\n",
      "[CV] END batch_size=32, hidden_layers=1, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=64; total time=   1.0s\n",
      "[CV] END batch_size=32, hidden_layers=1, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=64; total time=   1.0s\n",
      "[CV] END batch_size=32, hidden_layers=1, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=64; total time=   1.0s\n",
      "[CV] END batch_size=32, hidden_layers=1, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=128; total time=   1.0s\n",
      "[CV] END batch_size=32, hidden_layers=1, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=128; total time=   1.0s\n",
      "[CV] END batch_size=32, hidden_layers=1, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=128; total time=   1.0s\n",
      "[CV] END batch_size=32, hidden_layers=2, optimizer=adam, units=32; total time=   1.2s\n",
      "[CV] END batch_size=32, hidden_layers=2, optimizer=adam, units=32; total time=   1.2s\n",
      "[CV] END batch_size=32, hidden_layers=2, optimizer=adam, units=32; total time=   1.2s\n",
      "[CV] END batch_size=32, hidden_layers=2, optimizer=adam, units=64; total time=   1.2s\n",
      "[CV] END batch_size=32, hidden_layers=2, optimizer=adam, units=64; total time=   1.2s\n",
      "[CV] END batch_size=32, hidden_layers=2, optimizer=adam, units=64; total time=   1.2s\n",
      "[CV] END batch_size=32, hidden_layers=2, optimizer=adam, units=128; total time=   1.2s\n",
      "[CV] END batch_size=32, hidden_layers=2, optimizer=adam, units=128; total time=   1.2s\n",
      "[CV] END batch_size=32, hidden_layers=2, optimizer=adam, units=128; total time=   2.1s\n",
      "[CV] END batch_size=32, hidden_layers=2, optimizer=rmsprop, units=32; total time=   1.2s\n",
      "[CV] END batch_size=32, hidden_layers=2, optimizer=rmsprop, units=32; total time=   1.2s\n",
      "[CV] END batch_size=32, hidden_layers=2, optimizer=rmsprop, units=32; total time=   1.4s\n",
      "[CV] END batch_size=32, hidden_layers=2, optimizer=rmsprop, units=64; total time=   1.2s\n",
      "[CV] END batch_size=32, hidden_layers=2, optimizer=rmsprop, units=64; total time=   1.2s\n",
      "[CV] END batch_size=32, hidden_layers=2, optimizer=rmsprop, units=64; total time=   1.2s\n",
      "[CV] END batch_size=32, hidden_layers=2, optimizer=rmsprop, units=128; total time=   1.2s\n",
      "[CV] END batch_size=32, hidden_layers=2, optimizer=rmsprop, units=128; total time=   1.3s\n",
      "[CV] END batch_size=32, hidden_layers=2, optimizer=rmsprop, units=128; total time=   1.2s\n",
      "[CV] END batch_size=32, hidden_layers=2, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=32; total time=   1.2s\n",
      "[CV] END batch_size=32, hidden_layers=2, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=32; total time=   1.2s\n",
      "[CV] END batch_size=32, hidden_layers=2, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=32; total time=   1.2s\n",
      "[CV] END batch_size=32, hidden_layers=2, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=64; total time=   1.2s\n",
      "[CV] END batch_size=32, hidden_layers=2, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=64; total time=   1.2s\n",
      "[CV] END batch_size=32, hidden_layers=2, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=64; total time=   1.2s\n",
      "[CV] END batch_size=32, hidden_layers=2, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=128; total time=   1.3s\n",
      "[CV] END batch_size=32, hidden_layers=2, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=128; total time=   1.2s\n",
      "[CV] END batch_size=32, hidden_layers=2, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=128; total time=   1.2s\n",
      "[CV] END batch_size=32, hidden_layers=3, optimizer=adam, units=32; total time=   1.4s\n",
      "[CV] END batch_size=32, hidden_layers=3, optimizer=adam, units=32; total time=   1.4s\n",
      "[CV] END batch_size=32, hidden_layers=3, optimizer=adam, units=32; total time=   1.4s\n",
      "[CV] END batch_size=32, hidden_layers=3, optimizer=adam, units=64; total time=   1.4s\n",
      "[CV] END batch_size=32, hidden_layers=3, optimizer=adam, units=64; total time=   1.4s\n",
      "[CV] END batch_size=32, hidden_layers=3, optimizer=adam, units=64; total time=   1.4s\n",
      "[CV] END batch_size=32, hidden_layers=3, optimizer=adam, units=128; total time=   1.5s\n",
      "[CV] END batch_size=32, hidden_layers=3, optimizer=adam, units=128; total time=   1.5s\n",
      "[CV] END batch_size=32, hidden_layers=3, optimizer=adam, units=128; total time=   1.5s\n",
      "[CV] END batch_size=32, hidden_layers=3, optimizer=rmsprop, units=32; total time=   1.4s\n",
      "[CV] END batch_size=32, hidden_layers=3, optimizer=rmsprop, units=32; total time=   1.4s\n",
      "[CV] END batch_size=32, hidden_layers=3, optimizer=rmsprop, units=32; total time=   1.4s\n",
      "[CV] END batch_size=32, hidden_layers=3, optimizer=rmsprop, units=64; total time=   1.4s\n",
      "[CV] END batch_size=32, hidden_layers=3, optimizer=rmsprop, units=64; total time=   1.4s\n",
      "[CV] END batch_size=32, hidden_layers=3, optimizer=rmsprop, units=64; total time=   2.3s\n",
      "[CV] END batch_size=32, hidden_layers=3, optimizer=rmsprop, units=128; total time=   1.4s\n",
      "[CV] END batch_size=32, hidden_layers=3, optimizer=rmsprop, units=128; total time=   1.5s\n",
      "[CV] END batch_size=32, hidden_layers=3, optimizer=rmsprop, units=128; total time=   1.5s\n",
      "[CV] END batch_size=32, hidden_layers=3, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=32; total time=   1.4s\n",
      "[CV] END batch_size=32, hidden_layers=3, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=32; total time=   1.5s\n",
      "[CV] END batch_size=32, hidden_layers=3, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=32; total time=   1.5s\n",
      "[CV] END batch_size=32, hidden_layers=3, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=64; total time=   1.4s\n",
      "[CV] END batch_size=32, hidden_layers=3, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=64; total time=   1.4s\n",
      "[CV] END batch_size=32, hidden_layers=3, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=64; total time=   1.4s\n",
      "[CV] END batch_size=32, hidden_layers=3, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=128; total time=   1.5s\n",
      "[CV] END batch_size=32, hidden_layers=3, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=128; total time=   1.5s\n",
      "[CV] END batch_size=32, hidden_layers=3, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=128; total time=   1.4s\n",
      "[CV] END batch_size=32, hidden_layers=5, optimizer=adam, units=32; total time=   1.8s\n",
      "[CV] END batch_size=32, hidden_layers=5, optimizer=adam, units=32; total time=   1.8s\n",
      "[CV] END batch_size=32, hidden_layers=5, optimizer=adam, units=32; total time=   1.8s\n",
      "[CV] END batch_size=32, hidden_layers=5, optimizer=adam, units=64; total time=   1.8s\n",
      "[CV] END batch_size=32, hidden_layers=5, optimizer=adam, units=64; total time=   1.8s\n",
      "[CV] END batch_size=32, hidden_layers=5, optimizer=adam, units=64; total time=   2.0s\n",
      "[CV] END batch_size=32, hidden_layers=5, optimizer=adam, units=128; total time=   1.9s\n",
      "[CV] END batch_size=32, hidden_layers=5, optimizer=adam, units=128; total time=   1.9s\n",
      "[CV] END batch_size=32, hidden_layers=5, optimizer=adam, units=128; total time=   1.9s\n",
      "[CV] END batch_size=32, hidden_layers=5, optimizer=rmsprop, units=32; total time=   1.8s\n",
      "[CV] END batch_size=32, hidden_layers=5, optimizer=rmsprop, units=32; total time=   1.8s\n",
      "[CV] END batch_size=32, hidden_layers=5, optimizer=rmsprop, units=32; total time=   1.8s\n",
      "[CV] END batch_size=32, hidden_layers=5, optimizer=rmsprop, units=64; total time=   1.8s\n",
      "[CV] END batch_size=32, hidden_layers=5, optimizer=rmsprop, units=64; total time=   1.8s\n",
      "[CV] END batch_size=32, hidden_layers=5, optimizer=rmsprop, units=64; total time=   1.8s\n",
      "[CV] END batch_size=32, hidden_layers=5, optimizer=rmsprop, units=128; total time=   1.9s\n",
      "[CV] END batch_size=32, hidden_layers=5, optimizer=rmsprop, units=128; total time=   3.0s\n",
      "[CV] END batch_size=32, hidden_layers=5, optimizer=rmsprop, units=128; total time=   1.9s\n",
      "[CV] END batch_size=32, hidden_layers=5, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=32; total time=   1.8s\n",
      "[CV] END batch_size=32, hidden_layers=5, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=32; total time=   1.8s\n",
      "[CV] END batch_size=32, hidden_layers=5, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=32; total time=   1.8s\n",
      "[CV] END batch_size=32, hidden_layers=5, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=64; total time=   1.8s\n",
      "[CV] END batch_size=32, hidden_layers=5, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=64; total time=   1.8s\n",
      "[CV] END batch_size=32, hidden_layers=5, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=64; total time=   1.8s\n",
      "[CV] END batch_size=32, hidden_layers=5, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=128; total time=   1.9s\n",
      "[CV] END batch_size=32, hidden_layers=5, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=128; total time=   1.9s\n",
      "[CV] END batch_size=32, hidden_layers=5, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=128; total time=   1.9s\n",
      "[CV] END batch_size=64, hidden_layers=1, optimizer=adam, units=32; total time=   0.7s\n",
      "[CV] END batch_size=64, hidden_layers=1, optimizer=adam, units=32; total time=   0.8s\n",
      "[CV] END batch_size=64, hidden_layers=1, optimizer=adam, units=32; total time=   0.7s\n",
      "[CV] END batch_size=64, hidden_layers=1, optimizer=adam, units=64; total time=   0.7s\n",
      "[CV] END batch_size=64, hidden_layers=1, optimizer=adam, units=64; total time=   0.7s\n",
      "[CV] END batch_size=64, hidden_layers=1, optimizer=adam, units=64; total time=   0.8s\n",
      "[CV] END batch_size=64, hidden_layers=1, optimizer=adam, units=128; total time=   0.8s\n",
      "[CV] END batch_size=64, hidden_layers=1, optimizer=adam, units=128; total time=   0.8s\n",
      "[CV] END batch_size=64, hidden_layers=1, optimizer=adam, units=128; total time=   0.8s\n",
      "[CV] END batch_size=64, hidden_layers=1, optimizer=rmsprop, units=32; total time=   0.7s\n",
      "[CV] END batch_size=64, hidden_layers=1, optimizer=rmsprop, units=32; total time=   0.7s\n",
      "[CV] END batch_size=64, hidden_layers=1, optimizer=rmsprop, units=32; total time=   0.7s\n",
      "[CV] END batch_size=64, hidden_layers=1, optimizer=rmsprop, units=64; total time=   0.8s\n",
      "[CV] END batch_size=64, hidden_layers=1, optimizer=rmsprop, units=64; total time=   0.8s\n",
      "[CV] END batch_size=64, hidden_layers=1, optimizer=rmsprop, units=64; total time=   0.8s\n",
      "[CV] END batch_size=64, hidden_layers=1, optimizer=rmsprop, units=128; total time=   0.8s\n",
      "[CV] END batch_size=64, hidden_layers=1, optimizer=rmsprop, units=128; total time=   0.8s\n",
      "[CV] END batch_size=64, hidden_layers=1, optimizer=rmsprop, units=128; total time=   0.8s\n",
      "[CV] END batch_size=64, hidden_layers=1, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=32; total time=   0.8s\n",
      "[CV] END batch_size=64, hidden_layers=1, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=32; total time=   0.7s\n",
      "[CV] END batch_size=64, hidden_layers=1, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=32; total time=   0.8s\n",
      "[CV] END batch_size=64, hidden_layers=1, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=64; total time=   0.8s\n",
      "[CV] END batch_size=64, hidden_layers=1, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=64; total time=   0.8s\n",
      "[CV] END batch_size=64, hidden_layers=1, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=64; total time=   0.8s\n",
      "[CV] END batch_size=64, hidden_layers=1, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=128; total time=   0.8s\n",
      "[CV] END batch_size=64, hidden_layers=1, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=128; total time=   0.8s\n",
      "[CV] END batch_size=64, hidden_layers=1, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=128; total time=   0.8s\n",
      "[CV] END batch_size=64, hidden_layers=2, optimizer=adam, units=32; total time=   0.9s\n",
      "[CV] END batch_size=64, hidden_layers=2, optimizer=adam, units=32; total time=   0.9s\n",
      "[CV] END batch_size=64, hidden_layers=2, optimizer=adam, units=32; total time=   0.9s\n",
      "[CV] END batch_size=64, hidden_layers=2, optimizer=adam, units=64; total time=   1.0s\n",
      "[CV] END batch_size=64, hidden_layers=2, optimizer=adam, units=64; total time=   1.0s\n",
      "[CV] END batch_size=64, hidden_layers=2, optimizer=adam, units=64; total time=   0.9s\n",
      "[CV] END batch_size=64, hidden_layers=2, optimizer=adam, units=128; total time=   1.0s\n",
      "[CV] END batch_size=64, hidden_layers=2, optimizer=adam, units=128; total time=   1.0s\n",
      "[CV] END batch_size=64, hidden_layers=2, optimizer=adam, units=128; total time=   1.0s\n",
      "[CV] END batch_size=64, hidden_layers=2, optimizer=rmsprop, units=32; total time=   2.3s\n",
      "[CV] END batch_size=64, hidden_layers=2, optimizer=rmsprop, units=32; total time=   0.9s\n",
      "[CV] END batch_size=64, hidden_layers=2, optimizer=rmsprop, units=32; total time=   0.9s\n",
      "[CV] END batch_size=64, hidden_layers=2, optimizer=rmsprop, units=64; total time=   0.9s\n",
      "[CV] END batch_size=64, hidden_layers=2, optimizer=rmsprop, units=64; total time=   0.9s\n",
      "[CV] END batch_size=64, hidden_layers=2, optimizer=rmsprop, units=64; total time=   0.9s\n",
      "[CV] END batch_size=64, hidden_layers=2, optimizer=rmsprop, units=128; total time=   1.0s\n",
      "[CV] END batch_size=64, hidden_layers=2, optimizer=rmsprop, units=128; total time=   1.0s\n",
      "[CV] END batch_size=64, hidden_layers=2, optimizer=rmsprop, units=128; total time=   1.0s\n",
      "[CV] END batch_size=64, hidden_layers=2, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=32; total time=   0.9s\n",
      "[CV] END batch_size=64, hidden_layers=2, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=32; total time=   1.0s\n",
      "[CV] END batch_size=64, hidden_layers=2, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=32; total time=   0.9s\n",
      "[CV] END batch_size=64, hidden_layers=2, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=64; total time=   1.0s\n",
      "[CV] END batch_size=64, hidden_layers=2, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=64; total time=   0.9s\n",
      "[CV] END batch_size=64, hidden_layers=2, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=64; total time=   1.0s\n",
      "[CV] END batch_size=64, hidden_layers=2, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=128; total time=   1.0s\n",
      "[CV] END batch_size=64, hidden_layers=2, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=128; total time=   1.0s\n",
      "[CV] END batch_size=64, hidden_layers=2, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=128; total time=   1.0s\n",
      "[CV] END batch_size=64, hidden_layers=3, optimizer=adam, units=32; total time=   1.1s\n",
      "[CV] END batch_size=64, hidden_layers=3, optimizer=adam, units=32; total time=   1.1s\n",
      "[CV] END batch_size=64, hidden_layers=3, optimizer=adam, units=32; total time=   1.1s\n",
      "[CV] END batch_size=64, hidden_layers=3, optimizer=adam, units=64; total time=   1.1s\n",
      "[CV] END batch_size=64, hidden_layers=3, optimizer=adam, units=64; total time=   1.1s\n",
      "[CV] END batch_size=64, hidden_layers=3, optimizer=adam, units=64; total time=   1.2s\n",
      "[CV] END batch_size=64, hidden_layers=3, optimizer=adam, units=128; total time=   1.2s\n",
      "[CV] END batch_size=64, hidden_layers=3, optimizer=adam, units=128; total time=   1.2s\n",
      "[CV] END batch_size=64, hidden_layers=3, optimizer=adam, units=128; total time=   1.2s\n",
      "[CV] END batch_size=64, hidden_layers=3, optimizer=rmsprop, units=32; total time=   1.1s\n",
      "[CV] END batch_size=64, hidden_layers=3, optimizer=rmsprop, units=32; total time=   1.1s\n",
      "[CV] END batch_size=64, hidden_layers=3, optimizer=rmsprop, units=32; total time=   1.1s\n",
      "[CV] END batch_size=64, hidden_layers=3, optimizer=rmsprop, units=64; total time=   1.1s\n",
      "[CV] END batch_size=64, hidden_layers=3, optimizer=rmsprop, units=64; total time=   1.1s\n",
      "[CV] END batch_size=64, hidden_layers=3, optimizer=rmsprop, units=64; total time=   1.2s\n",
      "[CV] END batch_size=64, hidden_layers=3, optimizer=rmsprop, units=128; total time=   1.2s\n",
      "[CV] END batch_size=64, hidden_layers=3, optimizer=rmsprop, units=128; total time=   1.2s\n",
      "[CV] END batch_size=64, hidden_layers=3, optimizer=rmsprop, units=128; total time=   1.2s\n",
      "[CV] END batch_size=64, hidden_layers=3, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=32; total time=   1.1s\n",
      "[CV] END batch_size=64, hidden_layers=3, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=32; total time=   1.1s\n",
      "[CV] END batch_size=64, hidden_layers=3, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=32; total time=   1.1s\n",
      "[CV] END batch_size=64, hidden_layers=3, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=64; total time=   1.2s\n",
      "[CV] END batch_size=64, hidden_layers=3, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=64; total time=   1.1s\n",
      "[CV] END batch_size=64, hidden_layers=3, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=64; total time=   1.1s\n",
      "[CV] END batch_size=64, hidden_layers=3, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=128; total time=   1.2s\n",
      "[CV] END batch_size=64, hidden_layers=3, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=128; total time=   1.2s\n",
      "[CV] END batch_size=64, hidden_layers=3, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=128; total time=   1.2s\n",
      "[CV] END batch_size=64, hidden_layers=5, optimizer=adam, units=32; total time=   1.5s\n",
      "[CV] END batch_size=64, hidden_layers=5, optimizer=adam, units=32; total time=   1.5s\n",
      "[CV] END batch_size=64, hidden_layers=5, optimizer=adam, units=32; total time=   1.5s\n",
      "[CV] END batch_size=64, hidden_layers=5, optimizer=adam, units=64; total time=   1.6s\n",
      "[CV] END batch_size=64, hidden_layers=5, optimizer=adam, units=64; total time=   3.3s\n",
      "[CV] END batch_size=64, hidden_layers=5, optimizer=adam, units=64; total time=   1.5s\n",
      "[CV] END batch_size=64, hidden_layers=5, optimizer=adam, units=128; total time=   1.6s\n",
      "[CV] END batch_size=64, hidden_layers=5, optimizer=adam, units=128; total time=   1.6s\n",
      "[CV] END batch_size=64, hidden_layers=5, optimizer=adam, units=128; total time=   1.6s\n",
      "[CV] END batch_size=64, hidden_layers=5, optimizer=rmsprop, units=32; total time=   1.5s\n",
      "[CV] END batch_size=64, hidden_layers=5, optimizer=rmsprop, units=32; total time=   1.5s\n",
      "[CV] END batch_size=64, hidden_layers=5, optimizer=rmsprop, units=32; total time=   1.5s\n",
      "[CV] END batch_size=64, hidden_layers=5, optimizer=rmsprop, units=64; total time=   1.6s\n",
      "[CV] END batch_size=64, hidden_layers=5, optimizer=rmsprop, units=64; total time=   1.5s\n",
      "[CV] END batch_size=64, hidden_layers=5, optimizer=rmsprop, units=64; total time=   1.6s\n",
      "[CV] END batch_size=64, hidden_layers=5, optimizer=rmsprop, units=128; total time=   1.7s\n",
      "[CV] END batch_size=64, hidden_layers=5, optimizer=rmsprop, units=128; total time=   1.6s\n",
      "[CV] END batch_size=64, hidden_layers=5, optimizer=rmsprop, units=128; total time=   1.7s\n",
      "[CV] END batch_size=64, hidden_layers=5, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=32; total time=   1.5s\n",
      "[CV] END batch_size=64, hidden_layers=5, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=32; total time=   1.5s\n",
      "[CV] END batch_size=64, hidden_layers=5, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=32; total time=   1.5s\n",
      "[CV] END batch_size=64, hidden_layers=5, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=64; total time=   1.5s\n",
      "[CV] END batch_size=64, hidden_layers=5, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=64; total time=   1.5s\n",
      "[CV] END batch_size=64, hidden_layers=5, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=64; total time=   1.6s\n",
      "[CV] END batch_size=64, hidden_layers=5, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=128; total time=   1.7s\n",
      "[CV] END batch_size=64, hidden_layers=5, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=128; total time=   1.6s\n",
      "[CV] END batch_size=64, hidden_layers=5, optimizer=<class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, units=128; total time=   1.6s\n",
      "    param_optimizer param_hidden_layers param_units param_batch_size  \\\n",
      "0               SGD                   2         128               16   \n",
      "1           rmsprop                   3         128               16   \n",
      "2               SGD                   5         128               32   \n",
      "3              adam                   3         128               32   \n",
      "4           rmsprop                   5         128               32   \n",
      "..              ...                 ...         ...              ...   \n",
      "103         rmsprop                   3          64               64   \n",
      "104            adam                   3          32               64   \n",
      "105            adam                   5          64               64   \n",
      "106             SGD                   5          32               64   \n",
      "107         rmsprop                   3          32               64   \n",
      "\n",
      "     mean_test_score  \n",
      "0           0.785045  \n",
      "1           0.783582  \n",
      "2           0.780723  \n",
      "3           0.779829  \n",
      "4           0.779598  \n",
      "..               ...  \n",
      "103         0.637896  \n",
      "104         0.612140  \n",
      "105         0.588239  \n",
      "106         0.581989  \n",
      "107         0.579521  \n",
      "\n",
      "[108 rows x 5 columns]\n",
      "Best Parameters: {'batch_size': 16, 'hidden_layers': 2, 'optimizer': <class 'keras.src.optimizers.legacy.gradient_descent.SGD'>, 'units': 128}\n",
      "Epoch 1/20\n",
      "1028/1028 [==============================] - 2s 1ms/step - loss: 0.5492 - accuracy: 0.7219\n",
      "Epoch 2/20\n",
      "1028/1028 [==============================] - 1s 1ms/step - loss: 0.4481 - accuracy: 0.7949\n",
      "Epoch 3/20\n",
      "1028/1028 [==============================] - 1s 1ms/step - loss: 0.4037 - accuracy: 0.8221\n",
      "Epoch 4/20\n",
      "1028/1028 [==============================] - 1s 1ms/step - loss: 0.3621 - accuracy: 0.8438\n",
      "Epoch 5/20\n",
      "1028/1028 [==============================] - 1s 1ms/step - loss: 0.3335 - accuracy: 0.8560\n",
      "Epoch 6/20\n",
      "1028/1028 [==============================] - 1s 1ms/step - loss: 0.3061 - accuracy: 0.8729\n",
      "Epoch 7/20\n",
      "1028/1028 [==============================] - 1s 1ms/step - loss: 0.2801 - accuracy: 0.8837\n",
      "Epoch 8/20\n",
      "1028/1028 [==============================] - 1s 1ms/step - loss: 0.2645 - accuracy: 0.8917\n",
      "Epoch 9/20\n",
      "1028/1028 [==============================] - 1s 1ms/step - loss: 0.2391 - accuracy: 0.9038\n",
      "Epoch 10/20\n",
      "1028/1028 [==============================] - 1s 1ms/step - loss: 0.2293 - accuracy: 0.9069\n",
      "Epoch 11/20\n",
      "1028/1028 [==============================] - 1s 1ms/step - loss: 0.2142 - accuracy: 0.9143\n",
      "Epoch 12/20\n",
      "1028/1028 [==============================] - 1s 1ms/step - loss: 0.1991 - accuracy: 0.9211\n",
      "Epoch 13/20\n",
      "1028/1028 [==============================] - 1s 1ms/step - loss: 0.1816 - accuracy: 0.9290\n",
      "Epoch 14/20\n",
      "1028/1028 [==============================] - 1s 1ms/step - loss: 0.1883 - accuracy: 0.9256\n",
      "Epoch 15/20\n",
      "1028/1028 [==============================] - 1s 1ms/step - loss: 0.1801 - accuracy: 0.9294\n",
      "Epoch 16/20\n",
      "1028/1028 [==============================] - 1s 1ms/step - loss: 0.1650 - accuracy: 0.9328\n",
      "Epoch 17/20\n",
      "1028/1028 [==============================] - 1s 1ms/step - loss: 0.1651 - accuracy: 0.9343\n",
      "Epoch 18/20\n",
      "1028/1028 [==============================] - 1s 1ms/step - loss: 0.1563 - accuracy: 0.9385\n",
      "Epoch 19/20\n",
      "1028/1028 [==============================] - 1s 1ms/step - loss: 0.1578 - accuracy: 0.9370\n",
      "Epoch 20/20\n",
      "1028/1028 [==============================] - 1s 1ms/step - loss: 0.1466 - accuracy: 0.9416\n",
      "75/75 [==============================] - 0s 952us/step\n",
      "Precision on test set: 0.38\n",
      "Recall on test set: 0.40\n"
     ]
    }
   ],
   "source": [
    "# Define your parameter grid\n",
    "param_grid = {\n",
    "    'optimizer': ['adam', 'rmsprop', tf.keras.optimizers.legacy.SGD],\n",
    "    'hidden_layers': [1, 2, 3, 5],\n",
    "    'units': [32, 64, 128],\n",
    "    'batch_size': [16, 32, 64]\n",
    "}\n",
    "\n",
    "# Wrap the Keras model using KerasClassifier\n",
    "model = KerasClassifier(build_fn=create_model, hidden_layers=1,units=32,verbose=0)\n",
    "\n",
    "# Define the F1 scorer\n",
    "f1_scorer = make_scorer(f1_score)\n",
    "\n",
    "# Perform grid search\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, scoring=f1_scorer, verbose=2)\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# Extract results and convert to DataFrame\n",
    "results_df = pd.DataFrame(grid_result.cv_results_)\n",
    "\n",
    "# Select relevant columns from the results DataFrame\n",
    "relevant_columns = ['param_optimizer', 'param_hidden_layers', 'param_units', 'param_batch_size', 'mean_test_score']\n",
    "results_df = results_df[relevant_columns]\n",
    "\n",
    "# Replace tf.keras.optimizers.legacy.SGD with 'SGD'\n",
    "results_df['param_optimizer'] = results_df['param_optimizer'].replace({tf.keras.optimizers.legacy.SGD: 'SGD'})\n",
    "\n",
    "# Sort the DataFrame by mean_test_score in descending order\n",
    "results_df.sort_values(by='mean_test_score', ascending=False, inplace=True)\n",
    "\n",
    "# Reset index\n",
    "results_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(results_df)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_result.best_params_\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "# Build the optimizer\n",
    "if best_params['optimizer'] == tf.keras.optimizers.legacy.SGD:\n",
    "    optimizer = tf.keras.optimizers.legacy.SGD()\n",
    "else:\n",
    "    optimizer = best_params['optimizer']\n",
    "\n",
    "# Train the model with the best parameters\n",
    "best_model = create_model(optimizer=optimizer, hidden_layers=best_params['hidden_layers'], \n",
    "                          units=best_params['units'], batch_size=best_params['batch_size'])\n",
    "best_model.fit(X_train, y_train, epochs=20, batch_size= best_params['batch_size'], verbose=1)\n",
    "\n",
    "# Getting predictions\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_pred_binary = np.where(y_pred > 0.5, 1, 0)\n",
    "\n",
    "# Calculate precision and recall\n",
    "precision = precision_score(y_test, y_pred_binary)\n",
    "recall = recall_score(y_test, y_pred_binary)\n",
    "\n",
    "print(f\"Precision on test set: {precision:.2f}\")\n",
    "print(f\"Recall on test set: {recall:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_optimizer</th>\n",
       "      <th>param_hidden_layers</th>\n",
       "      <th>param_units</th>\n",
       "      <th>param_batch_size</th>\n",
       "      <th>mean_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SGD</td>\n",
       "      <td>2</td>\n",
       "      <td>128</td>\n",
       "      <td>16</td>\n",
       "      <td>0.785045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rmsprop</td>\n",
       "      <td>3</td>\n",
       "      <td>128</td>\n",
       "      <td>16</td>\n",
       "      <td>0.783582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SGD</td>\n",
       "      <td>5</td>\n",
       "      <td>128</td>\n",
       "      <td>32</td>\n",
       "      <td>0.780723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>adam</td>\n",
       "      <td>3</td>\n",
       "      <td>128</td>\n",
       "      <td>32</td>\n",
       "      <td>0.779829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rmsprop</td>\n",
       "      <td>5</td>\n",
       "      <td>128</td>\n",
       "      <td>32</td>\n",
       "      <td>0.779598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>rmsprop</td>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>0.637896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>adam</td>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>64</td>\n",
       "      <td>0.612140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>adam</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>0.588239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>SGD</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>64</td>\n",
       "      <td>0.581989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>rmsprop</td>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>64</td>\n",
       "      <td>0.579521</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>108 rows  5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    param_optimizer param_hidden_layers param_units param_batch_size  \\\n",
       "0               SGD                   2         128               16   \n",
       "1           rmsprop                   3         128               16   \n",
       "2               SGD                   5         128               32   \n",
       "3              adam                   3         128               32   \n",
       "4           rmsprop                   5         128               32   \n",
       "..              ...                 ...         ...              ...   \n",
       "103         rmsprop                   3          64               64   \n",
       "104            adam                   3          32               64   \n",
       "105            adam                   5          64               64   \n",
       "106             SGD                   5          32               64   \n",
       "107         rmsprop                   3          32               64   \n",
       "\n",
       "     mean_test_score  \n",
       "0           0.785045  \n",
       "1           0.783582  \n",
       "2           0.780723  \n",
       "3           0.779829  \n",
       "4           0.779598  \n",
       "..               ...  \n",
       "103         0.637896  \n",
       "104         0.612140  \n",
       "105         0.588239  \n",
       "106         0.581989  \n",
       "107         0.579521  \n",
       "\n",
       "[108 rows x 5 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_summary_1 = results_df\n",
    "grid_summary_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 16,\n",
       " 'hidden_layers': 2,\n",
       " 'optimizer': keras.src.optimizers.legacy.gradient_descent.SGD,\n",
       " 'units': 128}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params_1= best_params\n",
    "best_params_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as it seems that both units and batch size the higher the better we will fix the optimizer = SGD and hidden layers = 2 and will test again higher units and smaller batch sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "[CV] END ............................batch_size=4, units=128; total time=   5.1s\n",
      "[CV] END ............................batch_size=4, units=128; total time=   5.4s\n",
      "[CV] END ............................batch_size=4, units=128; total time=   5.3s\n",
      "[CV] END ............................batch_size=4, units=180; total time=   5.1s\n",
      "[CV] END ............................batch_size=4, units=180; total time=   4.9s\n",
      "[CV] END ............................batch_size=4, units=180; total time=   5.0s\n",
      "[CV] END ............................batch_size=4, units=220; total time=   5.2s\n",
      "[CV] END ............................batch_size=4, units=220; total time=   5.1s\n",
      "[CV] END ............................batch_size=4, units=220; total time=   5.2s\n",
      "[CV] END ............................batch_size=4, units=250; total time=   5.4s\n",
      "[CV] END ............................batch_size=4, units=250; total time=   5.6s\n",
      "[CV] END ............................batch_size=4, units=250; total time=   5.6s\n",
      "[CV] END ............................batch_size=8, units=128; total time=   3.0s\n",
      "[CV] END ............................batch_size=8, units=128; total time=   3.0s\n",
      "[CV] END ............................batch_size=8, units=128; total time=   2.9s\n",
      "[CV] END ............................batch_size=8, units=180; total time=   3.2s\n",
      "[CV] END ............................batch_size=8, units=180; total time=   3.0s\n",
      "[CV] END ............................batch_size=8, units=180; total time=   5.5s\n",
      "[CV] END ............................batch_size=8, units=220; total time=   3.4s\n",
      "[CV] END ............................batch_size=8, units=220; total time=   3.3s\n",
      "[CV] END ............................batch_size=8, units=220; total time=   3.2s\n",
      "[CV] END ............................batch_size=8, units=250; total time=   3.3s\n",
      "[CV] END ............................batch_size=8, units=250; total time=   3.4s\n",
      "[CV] END ............................batch_size=8, units=250; total time=   3.5s\n",
      "[CV] END ...........................batch_size=16, units=128; total time=   2.1s\n",
      "[CV] END ...........................batch_size=16, units=128; total time=   2.0s\n",
      "[CV] END ...........................batch_size=16, units=128; total time=   2.1s\n",
      "[CV] END ...........................batch_size=16, units=180; total time=   2.1s\n",
      "[CV] END ...........................batch_size=16, units=180; total time=   2.1s\n",
      "[CV] END ...........................batch_size=16, units=180; total time=   2.1s\n",
      "[CV] END ...........................batch_size=16, units=220; total time=   2.3s\n",
      "[CV] END ...........................batch_size=16, units=220; total time=   2.2s\n",
      "[CV] END ...........................batch_size=16, units=220; total time=   2.3s\n",
      "[CV] END ...........................batch_size=16, units=250; total time=   2.3s\n",
      "[CV] END ...........................batch_size=16, units=250; total time=   2.3s\n",
      "[CV] END ...........................batch_size=16, units=250; total time=   2.3s\n",
      "   param_units param_batch_size  mean_test_score\n",
      "0          250               16         0.820181\n",
      "1          180               16         0.814218\n",
      "2          220               16         0.810200\n",
      "3          128               16         0.808238\n",
      "4          220                8         0.784369\n",
      "5          180                8         0.783319\n",
      "6          128                8         0.777452\n",
      "7          250                8         0.768291\n",
      "8          128                4         0.745246\n",
      "9          220                4         0.739598\n",
      "10         250                4         0.737688\n",
      "11         180                4         0.730197\n",
      "Best Parameters: {'batch_size': 16, 'units': 250}\n",
      "Epoch 1/20\n",
      "1028/1028 [==============================] - 2s 1ms/step - loss: 0.5164 - accuracy: 0.7490\n",
      "Epoch 2/20\n",
      "1028/1028 [==============================] - 1s 1ms/step - loss: 0.3919 - accuracy: 0.8252\n",
      "Epoch 3/20\n",
      "1028/1028 [==============================] - 1s 1ms/step - loss: 0.3243 - accuracy: 0.8614\n",
      "Epoch 4/20\n",
      "1028/1028 [==============================] - 1s 1ms/step - loss: 0.2745 - accuracy: 0.8887\n",
      "Epoch 5/20\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.2339 - accuracy: 0.9051\n",
      "Epoch 6/20\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.2077 - accuracy: 0.9178\n",
      "Epoch 7/20\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.1890 - accuracy: 0.9252\n",
      "Epoch 8/20\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.1700 - accuracy: 0.9329\n",
      "Epoch 9/20\n",
      "1028/1028 [==============================] - 2s 1ms/step - loss: 0.1503 - accuracy: 0.9419\n",
      "Epoch 10/20\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.1387 - accuracy: 0.9483\n",
      "Epoch 11/20\n",
      "1028/1028 [==============================] - 2s 1ms/step - loss: 0.1265 - accuracy: 0.9485\n",
      "Epoch 12/20\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.1232 - accuracy: 0.9532\n",
      "Epoch 13/20\n",
      "1028/1028 [==============================] - 2s 1ms/step - loss: 0.1301 - accuracy: 0.9503\n",
      "Epoch 14/20\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.1192 - accuracy: 0.9546\n",
      "Epoch 15/20\n",
      "1028/1028 [==============================] - 2s 1ms/step - loss: 0.1033 - accuracy: 0.9611\n",
      "Epoch 16/20\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.0971 - accuracy: 0.9629\n",
      "Epoch 17/20\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.0995 - accuracy: 0.9627\n",
      "Epoch 18/20\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.0977 - accuracy: 0.9630\n",
      "Epoch 19/20\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.0880 - accuracy: 0.9673\n",
      "Epoch 20/20\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.0822 - accuracy: 0.9691\n",
      "75/75 [==============================] - 0s 1ms/step\n",
      "Precision on test set: 0.43\n",
      "Recall on test set: 0.43\n"
     ]
    }
   ],
   "source": [
    "# Define your new parameter grid\n",
    "new_param_grid = {\n",
    "    'units': [128, 180, 220, 250],\n",
    "    'batch_size': [4,8,16]\n",
    "}\n",
    "\n",
    "# Wrap the Keras model using KerasClassifier\n",
    "new_model = KerasClassifier(build_fn=create_model, optimizer='SGD',units=128, hidden_layers=2, verbose=0)\n",
    "\n",
    "# Perform grid search\n",
    "new_grid = GridSearchCV(estimator=new_model, param_grid=new_param_grid, cv=3, scoring=f1_scorer, verbose=2)\n",
    "new_grid_result = new_grid.fit(X_train, y_train)\n",
    "\n",
    "# Extract results and convert to DataFrame\n",
    "new_results_df = pd.DataFrame(new_grid_result.cv_results_)\n",
    "\n",
    "# Select relevant columns from the results DataFrame\n",
    "new_relevant_columns = ['param_units', 'param_batch_size', 'mean_test_score']\n",
    "new_results_df = new_results_df[new_relevant_columns]\n",
    "\n",
    "# Sort the DataFrame by mean_test_score in descending order\n",
    "new_results_df.sort_values(by='mean_test_score', ascending=False, inplace=True)\n",
    "\n",
    "# Reset index\n",
    "new_results_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(new_results_df)\n",
    "\n",
    "# Get the best parameters\n",
    "new_best_params = new_grid_result.best_params_\n",
    "print(\"Best Parameters:\", new_best_params)\n",
    "\n",
    "# Train the model with the best parameters\n",
    "new_best_model = create_model(optimizer='SGD', hidden_layers=2, units=new_best_params['units'], batch_size=new_best_params['batch_size'])\n",
    "new_best_model.fit(X_train, y_train, epochs=20, batch_size= new_best_params['batch_size'], verbose=1)\n",
    "\n",
    "# Getting predictions\n",
    "new_y_pred = new_best_model.predict(X_test)\n",
    "new_y_pred_binary = np.where(new_y_pred > 0.5, 1, 0)\n",
    "\n",
    "# Calculate precision and recall\n",
    "new_precision = precision_score(y_test, new_y_pred_binary)\n",
    "new_recall = recall_score(y_test, new_y_pred_binary)\n",
    "\n",
    "print(f\"Precision on test set: {new_precision:.2f}\")\n",
    "print(f\"Recall on test set: {new_recall:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 16, 'units': 250}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params_2 = new_best_params\n",
    "best_params_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_units</th>\n",
       "      <th>param_batch_size</th>\n",
       "      <th>mean_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>250</td>\n",
       "      <td>16</td>\n",
       "      <td>0.820181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>180</td>\n",
       "      <td>16</td>\n",
       "      <td>0.814218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>220</td>\n",
       "      <td>16</td>\n",
       "      <td>0.810200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>128</td>\n",
       "      <td>16</td>\n",
       "      <td>0.808238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>220</td>\n",
       "      <td>8</td>\n",
       "      <td>0.784369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>180</td>\n",
       "      <td>8</td>\n",
       "      <td>0.783319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>128</td>\n",
       "      <td>8</td>\n",
       "      <td>0.777452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>250</td>\n",
       "      <td>8</td>\n",
       "      <td>0.768291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>128</td>\n",
       "      <td>4</td>\n",
       "      <td>0.745246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>220</td>\n",
       "      <td>4</td>\n",
       "      <td>0.739598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>250</td>\n",
       "      <td>4</td>\n",
       "      <td>0.737688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>180</td>\n",
       "      <td>4</td>\n",
       "      <td>0.730197</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   param_units param_batch_size  mean_test_score\n",
       "0          250               16         0.820181\n",
       "1          180               16         0.814218\n",
       "2          220               16         0.810200\n",
       "3          128               16         0.808238\n",
       "4          220                8         0.784369\n",
       "5          180                8         0.783319\n",
       "6          128                8         0.777452\n",
       "7          250                8         0.768291\n",
       "8          128                4         0.745246\n",
       "9          220                4         0.739598\n",
       "10         250                4         0.737688\n",
       "11         180                4         0.730197"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_summary_2= new_results_df\n",
    "grid_summary_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can already fix batch_size = 16 , but units has arrive to the maximum value so we will make a new grid with higher values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "[CV] END ..........................................units=250; total time=   1.8s\n",
      "[CV] END ..........................................units=250; total time=   1.8s\n",
      "[CV] END ..........................................units=250; total time=   1.8s\n",
      "[CV] END ..........................................units=280; total time=   1.8s\n",
      "[CV] END ..........................................units=280; total time=   2.0s\n",
      "[CV] END ..........................................units=280; total time=   1.8s\n",
      "[CV] END ..........................................units=310; total time=   2.3s\n",
      "[CV] END ..........................................units=310; total time=   2.4s\n",
      "[CV] END ..........................................units=310; total time=   2.3s\n",
      "  param_units  mean_test_score\n",
      "0         280         0.850210\n",
      "1         310         0.846807\n",
      "2         250         0.844207\n",
      "Best Parameters: {'units': 280}\n",
      "Epoch 1/20\n",
      "1028/1028 [==============================] - 3s 2ms/step - loss: 0.5115 - accuracy: 0.7538\n",
      "Epoch 2/20\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.3801 - accuracy: 0.8357\n",
      "Epoch 3/20\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.3040 - accuracy: 0.8719\n",
      "Epoch 4/20\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.2554 - accuracy: 0.8980\n",
      "Epoch 5/20\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.2154 - accuracy: 0.9150\n",
      "Epoch 6/20\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.1870 - accuracy: 0.9252\n",
      "Epoch 7/20\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.1664 - accuracy: 0.9366\n",
      "Epoch 8/20\n",
      "1028/1028 [==============================] - 2s 1ms/step - loss: 0.1561 - accuracy: 0.9374\n",
      "Epoch 9/20\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.1360 - accuracy: 0.9460\n",
      "Epoch 10/20\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.1245 - accuracy: 0.9535\n",
      "Epoch 11/20\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.1189 - accuracy: 0.9542\n",
      "Epoch 12/20\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.1180 - accuracy: 0.9553\n",
      "Epoch 13/20\n",
      "1028/1028 [==============================] - 2s 1ms/step - loss: 0.1059 - accuracy: 0.9594\n",
      "Epoch 14/20\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.1042 - accuracy: 0.9611\n",
      "Epoch 15/20\n",
      "1028/1028 [==============================] - 2s 1ms/step - loss: 0.0996 - accuracy: 0.9630\n",
      "Epoch 16/20\n",
      "1028/1028 [==============================] - 2s 1ms/step - loss: 0.0850 - accuracy: 0.9681\n",
      "Epoch 17/20\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.0927 - accuracy: 0.9662\n",
      "Epoch 18/20\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.0863 - accuracy: 0.9669\n",
      "Epoch 19/20\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.0800 - accuracy: 0.9709\n",
      "Epoch 20/20\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.0760 - accuracy: 0.9709\n",
      "75/75 [==============================] - 0s 1ms/step\n",
      "Precision on test set: 0.43\n",
      "Recall on test set: 0.42\n"
     ]
    }
   ],
   "source": [
    "# Define your new parameter grid\n",
    "new_param_grid = {\n",
    "    'units': [250,280,310],\n",
    "}\n",
    "\n",
    "# Wrap the Keras model using KerasClassifier\n",
    "new_model = KerasClassifier(build_fn=create_model, optimizer='SGD',units=128, hidden_layers=2, verbose=0)\n",
    "\n",
    "# Perform grid search\n",
    "new_grid = GridSearchCV(estimator=new_model, param_grid=new_param_grid, cv=3, scoring=f1_scorer, verbose=2)\n",
    "new_grid_result = new_grid.fit(X_train, y_train)\n",
    "\n",
    "# Extract results and convert to DataFrame\n",
    "new_results_df = pd.DataFrame(new_grid_result.cv_results_)\n",
    "\n",
    "# Select relevant columns from the results DataFrame\n",
    "new_relevant_columns = [ 'param_units', 'mean_test_score']\n",
    "new_results_df = new_results_df[new_relevant_columns]\n",
    "\n",
    "# Sort the DataFrame by mean_test_score in descending order\n",
    "new_results_df.sort_values(by='mean_test_score', ascending=False, inplace=True)\n",
    "\n",
    "# Reset index\n",
    "new_results_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(new_results_df)\n",
    "\n",
    "# Get the best parameters\n",
    "new_best_params = new_grid_result.best_params_\n",
    "print(\"Best Parameters:\", new_best_params)\n",
    "\n",
    "# Train the model with the best parameters\n",
    "new_best_model = create_model(optimizer='SGD', hidden_layers=2, units=new_best_params['units'], batch_size= 16)\n",
    "new_best_model.fit(X_train, y_train, epochs=20, batch_size= 16, verbose=1)\n",
    "\n",
    "# Getting predictions\n",
    "new_y_pred = new_best_model.predict(X_test)\n",
    "new_y_pred_binary = np.where(new_y_pred > 0.5, 1, 0)\n",
    "\n",
    "# Calculate precision and recall\n",
    "new_precision = precision_score(y_test, new_y_pred_binary)\n",
    "new_recall = recall_score(y_test, new_y_pred_binary)\n",
    "\n",
    "print(f\"Precision on test set: {new_precision:.2f}\")\n",
    "print(f\"Recall on test set: {new_recall:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'units': 280}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params_3 =new_best_params\n",
    "best_params_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_units</th>\n",
       "      <th>mean_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>280</td>\n",
       "      <td>0.850210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>310</td>\n",
       "      <td>0.846807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>250</td>\n",
       "      <td>0.844207</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  param_units  mean_test_score\n",
       "0         280         0.850210\n",
       "1         310         0.846807\n",
       "2         250         0.844207"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_summary_3 = new_results_df\n",
    "grid_summary_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can fix the following parameters:\n",
    " - optimizer = SGD\n",
    " - hidden layers = 2\n",
    " - batch size = 16\n",
    " - units = 28\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nex grid for testing: \n",
    " - learning_rate': [0.01, 0.1, 0.2],\n",
    "- 'dropout_rate': [0.1, 0.2, 0.3],\n",
    " - 'epochs': [20, 30, 40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we must create a new create_model function in order to set the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_lr(optimizer='SGD', learning_rate=0.1, hidden_layers=2, units=280, dropout_rate=None, epochs=20):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    for _ in range(hidden_layers):\n",
    "        model.add(Dense(units, activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        if dropout_rate is not None:\n",
    "            model.add(Dropout(dropout_rate))  # Add dropout layer with specified rate\n",
    "    \n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    if optimizer == 'SGD':\n",
    "        opt = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    elif optimizer == 'Adam':\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    elif optimizer == 'RMSprop':\n",
    "        opt = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported optimizer\")\n",
    "    \n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n",
      "[CV] END ....dropout_rate=0.1, epochs=20, learning_rate=0.01; total time=  14.8s\n",
      "[CV] END ....dropout_rate=0.1, epochs=20, learning_rate=0.01; total time=  18.3s\n",
      "[CV] END ....dropout_rate=0.1, epochs=20, learning_rate=0.01; total time=  18.7s\n",
      "[CV] END .....dropout_rate=0.1, epochs=20, learning_rate=0.1; total time=  18.6s\n",
      "[CV] END .....dropout_rate=0.1, epochs=20, learning_rate=0.1; total time=  19.1s\n",
      "[CV] END .....dropout_rate=0.1, epochs=20, learning_rate=0.1; total time=  19.1s\n",
      "[CV] END .....dropout_rate=0.1, epochs=20, learning_rate=0.2; total time=  20.0s\n",
      "[CV] END .....dropout_rate=0.1, epochs=20, learning_rate=0.2; total time=  19.8s\n",
      "[CV] END .....dropout_rate=0.1, epochs=20, learning_rate=0.2; total time=  20.4s\n",
      "[CV] END ....dropout_rate=0.1, epochs=30, learning_rate=0.01; total time=  29.7s\n",
      "[CV] END ....dropout_rate=0.1, epochs=30, learning_rate=0.01; total time=  29.8s\n",
      "[CV] END ....dropout_rate=0.1, epochs=30, learning_rate=0.01; total time=  30.4s\n",
      "[CV] END .....dropout_rate=0.1, epochs=30, learning_rate=0.1; total time=  30.4s\n",
      "[CV] END .....dropout_rate=0.1, epochs=30, learning_rate=0.1; total time=  27.6s\n",
      "[CV] END .....dropout_rate=0.1, epochs=30, learning_rate=0.1; total time=  28.7s\n",
      "[CV] END .....dropout_rate=0.1, epochs=30, learning_rate=0.2; total time=  30.0s\n",
      "[CV] END .....dropout_rate=0.1, epochs=30, learning_rate=0.2; total time=  29.2s\n",
      "[CV] END .....dropout_rate=0.1, epochs=30, learning_rate=0.2; total time=  30.1s\n",
      "[CV] END ....dropout_rate=0.1, epochs=40, learning_rate=0.01; total time=  40.5s\n",
      "[CV] END ....dropout_rate=0.1, epochs=40, learning_rate=0.01; total time=  41.2s\n",
      "[CV] END ....dropout_rate=0.1, epochs=40, learning_rate=0.01; total time=  37.4s\n",
      "[CV] END .....dropout_rate=0.1, epochs=40, learning_rate=0.1; total time=  38.0s\n",
      "[CV] END .....dropout_rate=0.1, epochs=40, learning_rate=0.1; total time=  38.4s\n",
      "[CV] END .....dropout_rate=0.1, epochs=40, learning_rate=0.1; total time=  39.8s\n",
      "[CV] END ....dropout_rate=0.1, epochs=40, learning_rate=0.2; total time=446.5min\n",
      "[CV] END .....dropout_rate=0.1, epochs=40, learning_rate=0.2; total time=  42.8s\n",
      "[CV] END .....dropout_rate=0.1, epochs=40, learning_rate=0.2; total time=  40.3s\n",
      "[CV] END ....dropout_rate=0.2, epochs=20, learning_rate=0.01; total time=  15.1s\n",
      "[CV] END ....dropout_rate=0.2, epochs=20, learning_rate=0.01; total time=  14.8s\n",
      "[CV] END ....dropout_rate=0.2, epochs=20, learning_rate=0.01; total time=  14.0s\n",
      "[CV] END .....dropout_rate=0.2, epochs=20, learning_rate=0.1; total time=  15.0s\n",
      "[CV] END .....dropout_rate=0.2, epochs=20, learning_rate=0.1; total time=  14.3s\n",
      "[CV] END .....dropout_rate=0.2, epochs=20, learning_rate=0.1; total time=  14.2s\n",
      "[CV] END .....dropout_rate=0.2, epochs=20, learning_rate=0.2; total time=  14.5s\n",
      "[CV] END .....dropout_rate=0.2, epochs=20, learning_rate=0.2; total time=  14.5s\n",
      "[CV] END .....dropout_rate=0.2, epochs=20, learning_rate=0.2; total time=  14.6s\n",
      "[CV] END ....dropout_rate=0.2, epochs=30, learning_rate=0.01; total time=  20.9s\n",
      "[CV] END ....dropout_rate=0.2, epochs=30, learning_rate=0.01; total time=  20.8s\n",
      "[CV] END ....dropout_rate=0.2, epochs=30, learning_rate=0.01; total time=  20.8s\n",
      "[CV] END .....dropout_rate=0.2, epochs=30, learning_rate=0.1; total time=  21.7s\n",
      "[CV] END .....dropout_rate=0.2, epochs=30, learning_rate=0.1; total time=  21.3s\n",
      "[CV] END .....dropout_rate=0.2, epochs=30, learning_rate=0.1; total time=  22.6s\n",
      "[CV] END .....dropout_rate=0.2, epochs=30, learning_rate=0.2; total time=  21.1s\n",
      "[CV] END .....dropout_rate=0.2, epochs=30, learning_rate=0.2; total time=  21.3s\n",
      "[CV] END .....dropout_rate=0.2, epochs=30, learning_rate=0.2; total time=  21.5s\n",
      "[CV] END ....dropout_rate=0.2, epochs=40, learning_rate=0.01; total time=  26.9s\n",
      "[CV] END ....dropout_rate=0.2, epochs=40, learning_rate=0.01; total time=  27.9s\n",
      "[CV] END ....dropout_rate=0.2, epochs=40, learning_rate=0.01; total time=  28.7s\n",
      "[CV] END .....dropout_rate=0.2, epochs=40, learning_rate=0.1; total time=  28.7s\n",
      "[CV] END .....dropout_rate=0.2, epochs=40, learning_rate=0.1; total time=  29.0s\n",
      "[CV] END .....dropout_rate=0.2, epochs=40, learning_rate=0.1; total time=  29.0s\n",
      "[CV] END .....dropout_rate=0.2, epochs=40, learning_rate=0.2; total time=  28.8s\n",
      "[CV] END .....dropout_rate=0.2, epochs=40, learning_rate=0.2; total time=  28.9s\n",
      "[CV] END .....dropout_rate=0.2, epochs=40, learning_rate=0.2; total time=  28.8s\n",
      "[CV] END ....dropout_rate=0.3, epochs=20, learning_rate=0.01; total time=  15.0s\n",
      "[CV] END ....dropout_rate=0.3, epochs=20, learning_rate=0.01; total time=  14.8s\n",
      "[CV] END ....dropout_rate=0.3, epochs=20, learning_rate=0.01; total time=  14.8s\n",
      "[CV] END .....dropout_rate=0.3, epochs=20, learning_rate=0.1; total time=  14.8s\n",
      "[CV] END .....dropout_rate=0.3, epochs=20, learning_rate=0.1; total time=  15.5s\n",
      "[CV] END .....dropout_rate=0.3, epochs=20, learning_rate=0.1; total time=  14.7s\n",
      "[CV] END .....dropout_rate=0.3, epochs=20, learning_rate=0.2; total time=24.3min\n",
      "[CV] END .....dropout_rate=0.3, epochs=20, learning_rate=0.2; total time=  17.2s\n",
      "[CV] END .....dropout_rate=0.3, epochs=20, learning_rate=0.2; total time=  17.7s\n",
      "[CV] END ....dropout_rate=0.3, epochs=30, learning_rate=0.01; total time=  24.9s\n",
      "[CV] END ....dropout_rate=0.3, epochs=30, learning_rate=0.01; total time=  26.9s\n",
      "[CV] END ....dropout_rate=0.3, epochs=30, learning_rate=0.01; total time=  23.7s\n",
      "[CV] END .....dropout_rate=0.3, epochs=30, learning_rate=0.1; total time=  23.1s\n",
      "[CV] END .....dropout_rate=0.3, epochs=30, learning_rate=0.1; total time=  22.6s\n",
      "[CV] END .....dropout_rate=0.3, epochs=30, learning_rate=0.1; total time=  22.8s\n",
      "[CV] END .....dropout_rate=0.3, epochs=30, learning_rate=0.2; total time=  22.8s\n",
      "[CV] END .....dropout_rate=0.3, epochs=30, learning_rate=0.2; total time=  23.3s\n",
      "[CV] END .....dropout_rate=0.3, epochs=30, learning_rate=0.2; total time=  23.1s\n",
      "[CV] END ....dropout_rate=0.3, epochs=40, learning_rate=0.01; total time=  30.2s\n",
      "[CV] END ....dropout_rate=0.3, epochs=40, learning_rate=0.01; total time=  29.9s\n",
      "[CV] END ....dropout_rate=0.3, epochs=40, learning_rate=0.01; total time=  30.0s\n",
      "[CV] END .....dropout_rate=0.3, epochs=40, learning_rate=0.1; total time=  30.4s\n",
      "[CV] END .....dropout_rate=0.3, epochs=40, learning_rate=0.1; total time=  30.9s\n",
      "[CV] END .....dropout_rate=0.3, epochs=40, learning_rate=0.1; total time=  30.4s\n",
      "[CV] END .....dropout_rate=0.3, epochs=40, learning_rate=0.2; total time=  30.5s\n",
      "[CV] END .....dropout_rate=0.3, epochs=40, learning_rate=0.2; total time=  30.8s\n",
      "[CV] END .....dropout_rate=0.3, epochs=40, learning_rate=0.2; total time=  30.9s\n",
      "   param_learning_rate param_dropout_rate param_epochs  mean_test_score\n",
      "0                  0.1                0.3           40         0.947323\n",
      "1                  0.1                0.1           40         0.945208\n",
      "2                  0.2                0.1           40         0.944951\n",
      "3                 0.01                0.1           40         0.944536\n",
      "4                  0.2                0.2           40         0.944026\n",
      "5                  0.1                0.1           30         0.943448\n",
      "6                  0.2                0.1           30         0.943306\n",
      "7                  0.1                0.1           20         0.942507\n",
      "8                  0.2                0.1           20         0.941493\n",
      "9                  0.1                0.2           40         0.940585\n",
      "10                0.01                0.1           20         0.940285\n",
      "11                 0.2                0.2           30         0.939932\n",
      "12                 0.1                0.2           30         0.938822\n",
      "13                 0.2                0.3           40         0.938281\n",
      "14                0.01                0.1           30         0.938164\n",
      "15                 0.2                0.3           30         0.936073\n",
      "16                 0.1                0.2           20         0.935426\n",
      "17                 0.2                0.2           20         0.934624\n",
      "18                0.01                0.2           30         0.934503\n",
      "19                0.01                0.2           40         0.933419\n",
      "20                0.01                0.3           40         0.933165\n",
      "21                0.01                0.3           30         0.931675\n",
      "22                 0.2                0.3           20         0.931135\n",
      "23                0.01                0.2           20         0.927525\n",
      "24                 0.1                0.3           20         0.924055\n",
      "25                 0.1                0.3           30         0.923125\n",
      "26                0.01                0.3           20         0.919882\n",
      "Best Parameters: {'dropout_rate': 0.3, 'epochs': 40, 'learning_rate': 0.1}\n",
      "Epoch 1/40\n",
      "1028/1028 [==============================] - 3s 2ms/step - loss: 0.6515 - accuracy: 0.7017\n",
      "Epoch 2/40\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.5444 - accuracy: 0.7484\n",
      "Epoch 3/40\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.4674 - accuracy: 0.7910\n",
      "Epoch 4/40\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.4169 - accuracy: 0.8171\n",
      "Epoch 5/40\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.3687 - accuracy: 0.8381\n",
      "Epoch 6/40\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.3342 - accuracy: 0.8608\n",
      "Epoch 7/40\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.2964 - accuracy: 0.8794\n",
      "Epoch 8/40\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.2748 - accuracy: 0.8897\n",
      "Epoch 9/40\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.2667 - accuracy: 0.8915\n",
      "Epoch 10/40\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.2346 - accuracy: 0.9072\n",
      "Epoch 11/40\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.2184 - accuracy: 0.9126\n",
      "Epoch 12/40\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.2100 - accuracy: 0.9171\n",
      "Epoch 13/40\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.1934 - accuracy: 0.9246\n",
      "Epoch 14/40\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.1841 - accuracy: 0.9273\n",
      "Epoch 15/40\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.1756 - accuracy: 0.9329\n",
      "Epoch 16/40\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.1633 - accuracy: 0.9382\n",
      "Epoch 17/40\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.1578 - accuracy: 0.9386\n",
      "Epoch 18/40\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.1529 - accuracy: 0.9436\n",
      "Epoch 19/40\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.1495 - accuracy: 0.9434\n",
      "Epoch 20/40\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.1358 - accuracy: 0.9464\n",
      "Epoch 21/40\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.1326 - accuracy: 0.9486\n",
      "Epoch 22/40\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.1301 - accuracy: 0.9506\n",
      "Epoch 23/40\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.1304 - accuracy: 0.9526\n",
      "Epoch 24/40\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.1177 - accuracy: 0.9564\n",
      "Epoch 25/40\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.1203 - accuracy: 0.9545\n",
      "Epoch 26/40\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.1155 - accuracy: 0.9557\n",
      "Epoch 27/40\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.1081 - accuracy: 0.9591\n",
      "Epoch 28/40\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.1095 - accuracy: 0.9608\n",
      "Epoch 29/40\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.1133 - accuracy: 0.9571\n",
      "Epoch 30/40\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.1042 - accuracy: 0.9597\n",
      "Epoch 31/40\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.0988 - accuracy: 0.9638\n",
      "Epoch 32/40\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.0981 - accuracy: 0.9641\n",
      "Epoch 33/40\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.0939 - accuracy: 0.9661\n",
      "Epoch 34/40\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.0863 - accuracy: 0.9695\n",
      "Epoch 35/40\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.0938 - accuracy: 0.9653\n",
      "Epoch 36/40\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.0928 - accuracy: 0.9654\n",
      "Epoch 37/40\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.0868 - accuracy: 0.9692\n",
      "Epoch 38/40\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.0817 - accuracy: 0.9693\n",
      "Epoch 39/40\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.0843 - accuracy: 0.9694\n",
      "Epoch 40/40\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.0819 - accuracy: 0.9686\n",
      "75/75 [==============================] - 0s 1ms/step\n",
      "Precision on test set: 0.43\n",
      "Recall on test set: 0.40\n"
     ]
    }
   ],
   "source": [
    "# Define your new parameter grid\n",
    "new_param_grid = {\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'dropout_rate': [0.1, 0.2, 0.3],\n",
    "    'epochs': [20, 30, 40]\n",
    "}\n",
    "# Wrap the Keras model using KerasClassifier\n",
    "new_model = KerasClassifier(build_fn=create_model_lr, optimizer='SGD', hidden_layers=2, units=280,dropout_rate=0.2, learning_rate=0.1, verbose=0)\n",
    "\n",
    "# Define the F1 scorer\n",
    "f1_scorer = make_scorer(f1_score)\n",
    "\n",
    "# Perform grid search\n",
    "new_grid = GridSearchCV(estimator=new_model, param_grid=new_param_grid, cv=3, scoring=f1_scorer, verbose=2)\n",
    "new_grid_result = new_grid.fit(X_train, y_train)\n",
    "\n",
    "# Extract results and convert to DataFrame\n",
    "new_results_df = pd.DataFrame(new_grid_result.cv_results_)\n",
    "\n",
    "# Select relevant columns from the results DataFrame\n",
    "new_relevant_columns = ['param_learning_rate', 'param_dropout_rate', 'param_epochs', 'mean_test_score']\n",
    "new_results_df = new_results_df[new_relevant_columns]\n",
    "\n",
    "# Sort the DataFrame by mean_test_score in descending order\n",
    "new_results_df.sort_values(by='mean_test_score', ascending=False, inplace=True)\n",
    "\n",
    "# Reset index\n",
    "new_results_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(new_results_df)\n",
    "\n",
    "# Get the best parameters\n",
    "new_best_params = new_grid_result.best_params_\n",
    "print(\"Best Parameters:\", new_best_params)\n",
    "\n",
    "# Train the model with the best parameters\n",
    "new_best_model = create_model_lr(optimizer='SGD', hidden_layers=2, units=280, \n",
    "                               learning_rate=new_best_params['learning_rate'], \n",
    "                               dropout_rate=new_best_params['dropout_rate'],\n",
    "                               epochs=new_best_params['epochs'])\n",
    "new_best_model.fit(X_train, y_train, epochs=new_best_params['epochs'], batch_size=16, verbose=1)\n",
    "\n",
    "# Getting predictions\n",
    "new_y_pred = new_best_model.predict(X_test)\n",
    "new_y_pred_binary = (new_y_pred > 0.5).astype(int)\n",
    "\n",
    "# Calculate precision and recall\n",
    "new_precision = precision_score(y_test, new_y_pred_binary)\n",
    "new_recall = recall_score(y_test, new_y_pred_binary)\n",
    "\n",
    "print(f\"Precision on test set: {new_precision:.2f}\")\n",
    "print(f\"Recall on test set: {new_recall:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dropout_rate': 0.3, 'epochs': 40, 'learning_rate': 0.1}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params_4 =new_best_params\n",
    "best_params_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_dropout_rate</th>\n",
       "      <th>param_epochs</th>\n",
       "      <th>mean_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>40</td>\n",
       "      <td>0.947323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>40</td>\n",
       "      <td>0.945208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>40</td>\n",
       "      <td>0.944951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.1</td>\n",
       "      <td>40</td>\n",
       "      <td>0.944536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>40</td>\n",
       "      <td>0.944026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>30</td>\n",
       "      <td>0.943448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>30</td>\n",
       "      <td>0.943306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>20</td>\n",
       "      <td>0.942507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>20</td>\n",
       "      <td>0.941493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>40</td>\n",
       "      <td>0.940585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.1</td>\n",
       "      <td>20</td>\n",
       "      <td>0.940285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>30</td>\n",
       "      <td>0.939932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>30</td>\n",
       "      <td>0.938822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>40</td>\n",
       "      <td>0.938281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.1</td>\n",
       "      <td>30</td>\n",
       "      <td>0.938164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>30</td>\n",
       "      <td>0.936073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20</td>\n",
       "      <td>0.935426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20</td>\n",
       "      <td>0.934624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.2</td>\n",
       "      <td>30</td>\n",
       "      <td>0.934503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.2</td>\n",
       "      <td>40</td>\n",
       "      <td>0.933419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.3</td>\n",
       "      <td>40</td>\n",
       "      <td>0.933165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.3</td>\n",
       "      <td>30</td>\n",
       "      <td>0.931675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>20</td>\n",
       "      <td>0.931135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20</td>\n",
       "      <td>0.927525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>20</td>\n",
       "      <td>0.924055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>30</td>\n",
       "      <td>0.923125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.3</td>\n",
       "      <td>20</td>\n",
       "      <td>0.919882</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   param_learning_rate param_dropout_rate param_epochs  mean_test_score\n",
       "0                  0.1                0.3           40         0.947323\n",
       "1                  0.1                0.1           40         0.945208\n",
       "2                  0.2                0.1           40         0.944951\n",
       "3                 0.01                0.1           40         0.944536\n",
       "4                  0.2                0.2           40         0.944026\n",
       "5                  0.1                0.1           30         0.943448\n",
       "6                  0.2                0.1           30         0.943306\n",
       "7                  0.1                0.1           20         0.942507\n",
       "8                  0.2                0.1           20         0.941493\n",
       "9                  0.1                0.2           40         0.940585\n",
       "10                0.01                0.1           20         0.940285\n",
       "11                 0.2                0.2           30         0.939932\n",
       "12                 0.1                0.2           30         0.938822\n",
       "13                 0.2                0.3           40         0.938281\n",
       "14                0.01                0.1           30         0.938164\n",
       "15                 0.2                0.3           30         0.936073\n",
       "16                 0.1                0.2           20         0.935426\n",
       "17                 0.2                0.2           20         0.934624\n",
       "18                0.01                0.2           30         0.934503\n",
       "19                0.01                0.2           40         0.933419\n",
       "20                0.01                0.3           40         0.933165\n",
       "21                0.01                0.3           30         0.931675\n",
       "22                 0.2                0.3           20         0.931135\n",
       "23                0.01                0.2           20         0.927525\n",
       "24                 0.1                0.3           20         0.924055\n",
       "25                 0.1                0.3           30         0.923125\n",
       "26                0.01                0.3           20         0.919882"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_summary_4 = new_results_df\n",
    "grid_summary_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have the learning rate of 0.1 as the best value. But we must retest epochs to see higher values and drop_out rate to test lower values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n",
      "[CV] END ........................dropout_rate=0.3, epochs=40; total time=  32.5s\n",
      "[CV] END ........................dropout_rate=0.3, epochs=40; total time=  31.3s\n",
      "[CV] END ........................dropout_rate=0.3, epochs=40; total time=  30.6s\n",
      "[CV] END ........................dropout_rate=0.3, epochs=50; total time=  40.2s\n",
      "[CV] END ........................dropout_rate=0.3, epochs=50; total time=  39.4s\n",
      "[CV] END ........................dropout_rate=0.3, epochs=50; total time=  41.5s\n",
      "[CV] END ........................dropout_rate=0.3, epochs=60; total time=  45.5s\n",
      "[CV] END ........................dropout_rate=0.3, epochs=60; total time=  47.0s\n",
      "[CV] END ........................dropout_rate=0.3, epochs=60; total time=  47.5s\n",
      "[CV] END ........................dropout_rate=0.4, epochs=40; total time=  32.5s\n",
      "[CV] END ........................dropout_rate=0.4, epochs=40; total time=  33.1s\n",
      "[CV] END ........................dropout_rate=0.4, epochs=40; total time=  31.8s\n",
      "[CV] END ........................dropout_rate=0.4, epochs=50; total time=  37.0s\n",
      "[CV] END ........................dropout_rate=0.4, epochs=50; total time=  40.0s\n",
      "[CV] END ........................dropout_rate=0.4, epochs=50; total time=  38.3s\n",
      "[CV] END ........................dropout_rate=0.4, epochs=60; total time=  46.1s\n",
      "[CV] END ........................dropout_rate=0.4, epochs=60; total time=  44.7s\n",
      "[CV] END ........................dropout_rate=0.4, epochs=60; total time=  45.1s\n",
      "[CV] END ........................dropout_rate=0.5, epochs=40; total time=  32.7s\n",
      "[CV] END ........................dropout_rate=0.5, epochs=40; total time=  31.3s\n",
      "[CV] END ........................dropout_rate=0.5, epochs=40; total time=  35.6s\n",
      "[CV] END ........................dropout_rate=0.5, epochs=50; total time=  38.6s\n",
      "[CV] END ........................dropout_rate=0.5, epochs=50; total time=  39.4s\n",
      "[CV] END ........................dropout_rate=0.5, epochs=50; total time=  39.6s\n",
      "[CV] END ........................dropout_rate=0.5, epochs=60; total time=  42.7s\n",
      "[CV] END ........................dropout_rate=0.5, epochs=60; total time=  54.1s\n",
      "[CV] END ........................dropout_rate=0.5, epochs=60; total time=  57.2s\n",
      "  param_dropout_rate param_epochs  mean_test_score\n",
      "0                0.3           60         0.948891\n",
      "1                0.3           40         0.943602\n",
      "2                0.5           60         0.941037\n",
      "3                0.4           40         0.940062\n",
      "4                0.3           50         0.939821\n",
      "5                0.4           60         0.938729\n",
      "6                0.4           50         0.938447\n",
      "7                0.5           50         0.932058\n",
      "8                0.5           40         0.928764\n",
      "Best Parameters: {'dropout_rate': 0.3, 'epochs': 60}\n",
      "Epoch 1/60\n",
      "1028/1028 [==============================] - 3s 2ms/step - loss: 0.6468 - accuracy: 0.6999\n",
      "Epoch 2/60\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.5358 - accuracy: 0.7557\n",
      "Epoch 3/60\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.4678 - accuracy: 0.7916\n",
      "Epoch 4/60\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.4141 - accuracy: 0.8184\n",
      "Epoch 5/60\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.3765 - accuracy: 0.8391\n",
      "Epoch 6/60\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.3405 - accuracy: 0.8594\n",
      "Epoch 7/60\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.3064 - accuracy: 0.8725\n",
      "Epoch 8/60\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.2890 - accuracy: 0.8797\n",
      "Epoch 9/60\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.2684 - accuracy: 0.8921\n",
      "Epoch 10/60\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.2520 - accuracy: 0.8966\n",
      "Epoch 11/60\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.2251 - accuracy: 0.9101\n",
      "Epoch 12/60\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.2139 - accuracy: 0.9148\n",
      "Epoch 13/60\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.2031 - accuracy: 0.9205\n",
      "Epoch 14/60\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.1904 - accuracy: 0.9251\n",
      "Epoch 15/60\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.1798 - accuracy: 0.9303\n",
      "Epoch 16/60\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.1687 - accuracy: 0.9343\n",
      "Epoch 17/60\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.1586 - accuracy: 0.9380\n",
      "Epoch 18/60\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.1502 - accuracy: 0.9431\n",
      "Epoch 19/60\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.1466 - accuracy: 0.9416\n",
      "Epoch 20/60\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.1507 - accuracy: 0.9405\n",
      "Epoch 21/60\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.1348 - accuracy: 0.9481\n",
      "Epoch 22/60\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.1368 - accuracy: 0.9469\n",
      "Epoch 23/60\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.1262 - accuracy: 0.9510\n",
      "Epoch 24/60\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.1171 - accuracy: 0.9554\n",
      "Epoch 25/60\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.1278 - accuracy: 0.9517\n",
      "Epoch 26/60\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.1115 - accuracy: 0.9579\n",
      "Epoch 27/60\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.1107 - accuracy: 0.9588\n",
      "Epoch 28/60\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.1162 - accuracy: 0.9564\n",
      "Epoch 29/60\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.1054 - accuracy: 0.9622\n",
      "Epoch 30/60\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.1088 - accuracy: 0.9606\n",
      "Epoch 31/60\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.1048 - accuracy: 0.9622\n",
      "Epoch 32/60\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.0994 - accuracy: 0.9627\n",
      "Epoch 33/60\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.0993 - accuracy: 0.9657\n",
      "Epoch 34/60\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.0891 - accuracy: 0.9675\n",
      "Epoch 35/60\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.0937 - accuracy: 0.9653\n",
      "Epoch 36/60\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.0879 - accuracy: 0.9672\n",
      "Epoch 37/60\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.0870 - accuracy: 0.9685\n",
      "Epoch 38/60\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.0884 - accuracy: 0.9677\n",
      "Epoch 39/60\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.0835 - accuracy: 0.9694\n",
      "Epoch 40/60\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.0856 - accuracy: 0.9671\n",
      "Epoch 41/60\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.0807 - accuracy: 0.9693\n",
      "Epoch 42/60\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.0845 - accuracy: 0.9694\n",
      "Epoch 43/60\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.0817 - accuracy: 0.9700\n",
      "Epoch 44/60\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.0756 - accuracy: 0.9727\n",
      "Epoch 45/60\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.0747 - accuracy: 0.9726\n",
      "Epoch 46/60\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.0753 - accuracy: 0.9740\n",
      "Epoch 47/60\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.0761 - accuracy: 0.9723\n",
      "Epoch 48/60\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.0756 - accuracy: 0.9740\n",
      "Epoch 49/60\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.0761 - accuracy: 0.9728\n",
      "Epoch 50/60\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.0729 - accuracy: 0.9745\n",
      "Epoch 51/60\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.0707 - accuracy: 0.9736\n",
      "Epoch 52/60\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.0751 - accuracy: 0.9733\n",
      "Epoch 53/60\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.0731 - accuracy: 0.9733\n",
      "Epoch 54/60\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.0651 - accuracy: 0.9754\n",
      "Epoch 55/60\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.0723 - accuracy: 0.9742\n",
      "Epoch 56/60\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.0732 - accuracy: 0.9734\n",
      "Epoch 57/60\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.0650 - accuracy: 0.9770\n",
      "Epoch 58/60\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.0723 - accuracy: 0.9742\n",
      "Epoch 59/60\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.0609 - accuracy: 0.9784\n",
      "Epoch 60/60\n",
      "1028/1028 [==============================] - 2s 2ms/step - loss: 0.0653 - accuracy: 0.9762\n",
      "75/75 [==============================] - 0s 1ms/step\n",
      "Precision on test set: 0.44\n",
      "Recall on test set: 0.38\n"
     ]
    }
   ],
   "source": [
    "# Define your new parameter grid\n",
    "new_param_grid = {\n",
    "    'dropout_rate': [0.3, 0.4, 0.5],\n",
    "    'epochs': [40, 50, 60]\n",
    "}\n",
    "\n",
    "# Wrap the Keras model using KerasClassifier\n",
    "new_model = KerasClassifier(build_fn=create_model_lr, optimizer='SGD', hidden_layers=2, units=280, dropout_rate=0.2, learning_rate=0.1, verbose=0)\n",
    "\n",
    "# Define the F1 scorer\n",
    "f1_scorer = make_scorer(f1_score)\n",
    "\n",
    "# Perform grid search\n",
    "new_grid = GridSearchCV(estimator=new_model, param_grid=new_param_grid, cv=3, scoring=f1_scorer, verbose=2)\n",
    "new_grid_result = new_grid.fit(X_train, y_train)\n",
    "\n",
    "# Extract results and convert to DataFrame\n",
    "new_results_df = pd.DataFrame(new_grid_result.cv_results_)\n",
    "\n",
    "# Select relevant columns from the results DataFrame\n",
    "new_relevant_columns = ['param_dropout_rate', 'param_epochs', 'mean_test_score']\n",
    "new_results_df = new_results_df[new_relevant_columns]\n",
    "\n",
    "# Sort the DataFrame by mean_test_score in descending order\n",
    "new_results_df.sort_values(by='mean_test_score', ascending=False, inplace=True)\n",
    "\n",
    "# Reset index\n",
    "new_results_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(new_results_df)\n",
    "\n",
    "# Get the best parameters\n",
    "new_best_params = new_grid_result.best_params_\n",
    "print(\"Best Parameters:\", new_best_params)\n",
    "\n",
    "# Train the model with the best parameters\n",
    "new_best_model = create_model_lr(optimizer='SGD', hidden_layers=2, units=280, \n",
    "                               learning_rate=0.1, \n",
    "                               dropout_rate=new_best_params['dropout_rate'],\n",
    "                               epochs=new_best_params['epochs'])\n",
    "new_best_model.fit(X_train, y_train, epochs=new_best_params['epochs'], batch_size=16, verbose=1)\n",
    "\n",
    "# Getting predictions\n",
    "new_y_pred = new_best_model.predict(X_test)\n",
    "new_y_pred_binary = (new_y_pred > 0.5).astype(int)\n",
    "\n",
    "# Calculate precision and recall\n",
    "new_precision = precision_score(y_test, new_y_pred_binary)\n",
    "new_recall = recall_score(y_test, new_y_pred_binary)\n",
    "\n",
    "print(f\"Precision on test set: {new_precision:.2f}\")\n",
    "print(f\"Recall on test set: {new_recall:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dropout_rate': 0.3, 'epochs': 60}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params_5 =new_best_params\n",
    "best_params_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_dropout_rate</th>\n",
       "      <th>param_epochs</th>\n",
       "      <th>mean_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.3</td>\n",
       "      <td>60</td>\n",
       "      <td>0.948891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.3</td>\n",
       "      <td>40</td>\n",
       "      <td>0.943602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.5</td>\n",
       "      <td>60</td>\n",
       "      <td>0.941037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.4</td>\n",
       "      <td>40</td>\n",
       "      <td>0.940062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.3</td>\n",
       "      <td>50</td>\n",
       "      <td>0.939821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.4</td>\n",
       "      <td>60</td>\n",
       "      <td>0.938729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.4</td>\n",
       "      <td>50</td>\n",
       "      <td>0.938447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.5</td>\n",
       "      <td>50</td>\n",
       "      <td>0.932058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.5</td>\n",
       "      <td>40</td>\n",
       "      <td>0.928764</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  param_dropout_rate param_epochs  mean_test_score\n",
       "0                0.3           60         0.948891\n",
       "1                0.3           40         0.943602\n",
       "2                0.5           60         0.941037\n",
       "3                0.4           40         0.940062\n",
       "4                0.3           50         0.939821\n",
       "5                0.4           60         0.938729\n",
       "6                0.4           50         0.938447\n",
       "7                0.5           50         0.932058\n",
       "8                0.5           40         0.928764"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_summary_5 = new_results_df\n",
    "grid_summary_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "although epochs the higher the better it is not worth it in terms of computing time. If we look at the precision and recall in our testing, training data we have not objectively improved thus, we keep as best parameters combination:\n",
    "- dropout_rate =0.3\n",
    "- epochs = 40\n",
    "- learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <U>FINAL PARAMETERS:\n",
    "- dropout_rate =0.3\n",
    "- epochs = 40\n",
    "- learning_rate = 0.1\n",
    "- optimizer = SGD\n",
    "- hidden layers = 2\n",
    "- batch size = 16\n",
    "- units = 28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
